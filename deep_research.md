Designing a Domain-Neutral Sequential Insights Tool for MCP Excel Server
Executive Summary
In this report, we outline the design of a domain-neutral “Sequential Insights” tool for an Excel-based MCP server. The goal is to empower an AI assistant (LLM) to perform step-by-step data analysis using deterministic server-provided insight primitives and a planning interface, without embedding any LLM logic on the server side. The MCP server (Go + Excelize) will offer a set of streaming-friendly analysis functions (e.g. aggregations, trend analysis, outlier detection) and a planning mechanism to chain these functions. Meanwhile, the LLM client will orchestrate the workflow: asking clarifying questions, calling the primitive tools in sequence, and narrating results to the user. This approach mirrors recent “sequential thinking” MCP patterns where the server guides multi-step reasoning with tool recommendations[1][2]. By decoupling logic, the server guarantees deterministic, bounded computations under strict memory/time limits, and the LLM ensures context, interpretation, and user-facing narrative.
Key Objectives: We identify a minimal set of insight primitives that generalize across business intelligence, finance, and data analytics workflows. These include: identifying column roles (measure vs dimension vs time), basic aggregations and partitioning, time-series trend and period-over-period comparisons, variance attribution (decomposing metric changes), composition or mix analysis, concentration metrics, exception/outlier flags, and data quality checks. For each, we define robust algorithms that can operate in a streaming fashion (row-by-row processing to handle large Excel sheets without exhausting memory). We also propose a planning schema (“Sequential Insights”) by which the server can suggest next analysis steps or clarifying questions in a domain-neutral way. This schema will include a structure for iterative “Insight Cards” – concise findings each with a finding statement, quantitative impact, evidence snippet from data, stated assumptions, and a suggested next action. Finally, we delineate the responsibilities of client vs server to ensure accuracy (e.g. server returns exact figures; LLM interprets them), safety (server enforces allowed operations and data limits; LLM handles user context and phrasing), and usability (clear prompts and fallback behaviors).
Summary of Approach: The Sequential Insights tool will first infer the dataset’s basic structure (e.g. detect time column and measure columns) to reduce ambiguity. It will then enter a planning loop: at each step, the server suggests either a direct insight or a question/tool to pursue, and the LLM uses that to decide the next action. The design emphasizes conservative, assumption-aware analysis – e.g. highlighting only statistically significant outliers to avoid false alarms, flagging data quality issues before analyzing trends, and choosing appropriate comparison windows (week-over-week, month-over-month, etc.) based on the time grain of data. The output to the end-user would be a series of Insight Cards that tell a coherent story about the data (for example, a trend card: “Sales increased 20% MoM in July, reaching $1.2M[3][4]”, with evidence and next steps). Throughout, the server’s streaming algorithms will handle large files by processing iteratively (using Excelize’s row iterator and streaming writer to avoid loading entire sheets in memory), and will apply caps (like focusing on top N categories for concentration metrics) to ensure performance. This document details the primitives, algorithms (with pseudocode), planning schema (with examples), client/server division of labor, insight card templates, and a discussion of risks and trade-offs. We also include references to relevant open-source MCP servers and BI literature to validate the design choices.
Primitives & Algorithms for Insights
This section catalogs the core analytical primitives the server will provide, along with their definitions, usage contexts, assumptions, and streaming-safe algorithm outlines. These primitives are domain-neutral – applicable to any tabular dataset (e.g. financial reports, sales data, operational metrics) – and designed to work within strict memory and time constraints by processing data in one pass when possible. We highlight how each primitive avoids false positives and handles edge cases. Pseudocode and complexity notes are included for clarity.
1. Column Role Inference
Definition & Purpose: Identify the semantic role of each column: e.g. Measure (numeric metric to aggregate), Dimension (categorical grouping or label), Time (date/time field indicating temporal order), ID (unique identifier or key), or Target (a special measure representing a goal/forecast to compare against actual). Correct role inference is critical as it determines which analyses are relevant – for example, time series trends require a time column; aggregations require measures with meaningful sums or averages.
Heuristics & Assumptions: The tool inspects column headers and a sample of values to guess roles. Common heuristics include: - If the header or values indicate a date or time format (e.g. “Date”, “Month”, values look like YYYY-MM-DD), classify as Time[5]. We also infer the time grain (periodicity) by analyzing date intervals (explained in the next section). - If values are numeric and continuous (varying over a wide range) and header suggests a quantity (e.g. “Sales”, “Total”, “Amount”), treat as Measure[5]. Measures usually can be aggregated meaningfully (sum, avg). - If values are textual or a limited set of categories (e.g. “Region”, “Product”), treat as Dimension – used to group or segment data[5]. High-cardinality text fields that look like freeform IDs (e.g. alphanumeric codes) might be classified as ID if each value is unique or nearly unique. Numeric columns with very few unique values (e.g. codes like 1, 2, 3) can also be dimensions despite being numbers. - A Target column might be identified if a header contains keywords like “Target”, “Goal”, “Budget”, or if there’s a pair of columns like Actual vs Target. In absence of explicit cues, the tool doesn’t assume a target; the LLM can clarify if the user’s objective implies one (e.g. “compare to plan” would hint at a target column).
Algorithm (pseudocode):

roles = {}
for each column:
  sample_values = first 100 non-null values
  header = column.name.lower()
  if is_date_format(sample_values) or header contains ["date","time","year","month","day"]:
      roles[column.name] = "Time"
  else if is_numeric(sample_values):
      unique_count = count_unique(sample_values)
      if unique_count == len(sample_values) and len(sample_values) > 20:
          # every value unique and many rows – likely an ID
          roles[column.name] = "ID"
      else if unique_count < 0.2 * len(sample_values):
          # low cardinality numeric, likely codes (dimension)
          roles[column.name] = "Dimension"
      else:
          roles[column.name] = "Measure"
  else:
      # non-numeric, non-date
      if looks_like_id(sample_values):
          roles[column.name] = "ID"
      else:
          roles[column.name] = "Dimension"
# (Optionally, detect "Target" if header indicates or matches a measure naming pattern)
Complexity: O(N) per column for sampling and simple stats. This runs in a streaming manner by reading a limited number of rows. The assumptions are that header names and value patterns reflect the role (which is common but not guaranteed). False positive risks: A numeric code (like store IDs) could be misclassified as measure; to mitigate, we check cardinality and possibly presence of non-numeric characters (if any). We favor classifying borderline cases as dimensions (safer, as treating a dimension as measure could lead to meaningless sums). The tool will surface uncertainties as clarifying questions (e.g. “Column Product_ID is numeric but looks like an identifier – exclude from aggregations?”).
2. Time Grain Detection & Comparison Windows
Definition: For a column identified as Time, determine its granularity (e.g. daily, weekly, monthly, quarterly) and set up appropriate comparison windows for analyzing changes (WoW = week-over-week, MoM = month-over-month, YoY = year-over-year, etc.). This allows the tool to choose a sensible baseline period when examining trends or changes over time. For instance, if data is monthly, a MoM change and YoY change are relevant; if weekly data, WoW is primary; if daily data, it might aggregate to weekly for WoW comparisons (to smooth out daily noise).
Detection Method: The tool examines the date series: - Uniform interval method: Compute the distribution of time gaps between consecutive records (after sorting by date). Identify the mode or common interval. For example, if intervals are mostly 1 day, grain is daily; 7 days -> weekly; ~30 days -> monthly; ~90 days -> quarterly; 1 year -> yearly. - Day-of-week/Month pattern: Alternatively, check the unique day-of-week values present. If multiple distinct weekdays appear (e.g. Mon, Wed, Fri), likely daily data[6]. If data points all share the same weekday (e.g. all Sundays), it might be weekly data reported on that weekday[6]. Similarly, if the day-of-month is constant (say all entries on the 1st of each month), it’s monthly. - Count vs range heuristic: Compare the number of data points to the span of time covered. If 12 points cover ~12 months, that’s monthly. If ~52 points cover a year, weekly. If ~365 cover a year, daily, etc.
Using a combination of these, the tool classifies the frequency. For example, an R implementation uses: “if at least 3 distinct weekdays present, classify as daily; if count of months == count of records, classify monthly; if all records are Sunday dates, classify weekly”[6]. We will implement a slightly more robust check of intervals and patterns.
Comparison Window Selection: Once grain is known, define how to compare periods: - Daily data: Use Week-over-Week (aggregate daily data into weekly buckets or compare the latest day vs the same weekday a week prior) as the primary comparison, since day-over-day can be too volatile. Also allow Month-to-date vs prior month-to-date if needed for monthly reporting context. - Weekly data: Use Week-over-Week (current week vs previous week). Possibly Year-over-Year if spanning multiple years (compare the current week of year vs same week last year) as secondary. - Monthly data: Use Month-over-Month (current month vs previous month), and Year-over-Year (this month vs same month last year) if data > 12 months. - Quarterly data: Use Quarter-over-Quarter and Year-over-Year. - Yearly data: Use Year-over-Year if multiple years, else no smaller window available (just show annual trend).
Fallback Rules: The tool will choose the largest meaningful comparison: - If a year-over-year comparison is possible (data spans > 1 year), include YoY, since it accounts for seasonal effects (e.g. July 2025 vs July 2024). - If not, use period-over-period of the native grain (MoM for monthly, QoQ for quarterly, etc.), provided at least 2 periods are present. - If only one period of data is present (e.g. only one month of data), the tool will not attempt a comparison and will instead flag that trend analysis is limited (perhaps ask the user for a broader date range). - Handle incomplete periods: e.g. if the current month is only partially complete compared to full last month, the insight generation will note that the comparison is partial (or it may compare the same number of days). The server can either adjust (compare first N days of each month) or let the LLM handle an explanation; likely we keep it simple: compare last full period and ignore the partial current period for change calculation, noting the cutoff.
Algorithm (pseudo):

dates = sorted(unique(time_col_values))
# determine grain by interval
deltas = [dates[i+1] - dates[i] for i in range(len(dates)-1)]
mode_gap = mode(deltas)
grain = classify_interval(mode_gap)  # e.g. 1 day -> 'daily', 7 days -> 'weekly', etc.
# Additional check:
if grain == 'weekly':
    weekday_set = unique([date.weekday() for date in dates])
    if len(weekday_set) > 1:
        grain = 'daily'  # multiple weekdays implies not strictly weekly intervals
# Determine comparison window:
if grain in ['daily','weekly']:
    primary_comp = 'WoW'
elif grain == 'monthly':
    primary_comp = 'MoM'
elif grain == 'quarterly':
    primary_comp = 'QoQ'
elif grain == 'yearly':
    primary_comp = 'YoY'
# Also allow YoY if span >= 1 year, etc.
allow_yoy = (max(dates) - min(dates) >= 365 days * 1)  # simple span check
Complexity: O(N) to scan dates and differences. This is efficient via streaming (we can track min, max, count, and perhaps use a small buffer or reservoir to detect patterns). Assumptions: Data is more or less regularly spaced; irregular missing dates could throw off mode detection. We handle some by logic (if intervals vary a lot or no clear mode, we might pick the largest interval as default or flag “unable to classify time frequency”). A limitation noted in one approach is assuming weeks align to Sundays[7] – our approach will not rely solely on that; we’ll simply detect a ~7-day pattern. False positives: Could misidentify daily vs weekly if data is sparse daily (e.g. only some days present). In such cases, we err on classifying as daily if any non-weekly pattern is seen, since daily analyses can be aggregated to weekly if needed. The planning stage can surface a clarifying question like “Data seems irregular over days – analyze as daily or weekly?” if uncertainty remains.
3. Aggregation & Partitioning
Definition: Basic group-by aggregations on measures by dimensions. This primitive computes summary statistics (sum, count, average, etc.) of one or more measure columns, optionally segmented by one or more dimension columns. It’s the foundational operation for many insights: e.g. total sales by region, average cost by category, count of records per status, etc. Partitioning the data by different dimensions helps identify where values are high or low.
Usage: The tool will perform aggregation as needed, often at the behest of another step (e.g. to find top contributors, we sum by category). We focus on streaming-safe computation: use one-pass calculation without storing all rows. Common functions (SUM, COUNT, MIN, MAX) are easily streamable; AVERAGE can be computed via running sum and count. More complex ones like median or percentiles are not computed here unless needed (those would require either multi-pass or approximation; not core to initial version). We also allow combination of dimensions (multi-level group-by) if the question demands (though the LLM can orchestrate drilling down sequentially as well).
Algorithm: We leverage Excelize’s row stream or similar iterator. For example, to sum Sales by Region:
result = {}  # dictionary of region -> sum
for each row in sheet:
    region = row["Region"]
    value = parse_number(row["Sales"])
    result[region] += value
# After loop, we have sum per region.
This can naturally be extended to multiple measures (compute multiple sums in parallel) or multiple dimensions (key by tuple (Dim1, Dim2)). The output will be a small JSON (or table) of aggregated results. The server can stream partial results if needed, but since grouping may have many keys, it might collect in memory until completion. We mitigate memory blow-up by possibly limiting cardinality: if a dimension has too many unique values (e.g. transaction IDs), grouping by it isn’t useful for high-level insights. The planning logic will avoid grouping by an ID column, focusing on true categorical dimensions. Also, we might implement a cap like “only report top 100 groups by value, group the rest as ‘Other’” for extremely high cardinality dimensions to keep payload small; however, this introduces approximation (we’d need a second pass to identify top N in streaming way – one approach is a min-heap of size N for top values).
Assumptions & Risks: We assume the measure columns are additive or aggregable. Some measures (like averages or ratios in source data) shouldn’t be summed – a data quality check might flag if summing a percentage field (unless weighted by something) is nonsensical. The tool might skip or warn on such cases (e.g. a column with many values between 0 and 1 could be percentage; summing 100 such entries to 80 might not be meaningful – an average would be better). These decisions could be guided by column naming or units. For V1, we will rely on the analyst (LLM) to use domain sense (perhaps via hints). False positives (e.g. summing an ID number column) are minimized by good role inference.
4. Change Detection & Variance Attribution
Definition: When comparing two scenarios (e.g. current vs prior period, or actual vs target), quantify the variance in the main KPI and attribute that change to different components or segments. This includes computing the absolute and percent change in a top-line metric and breaking down “what contributed to the change.” Two broad approaches: - By Segment: Identify which sub-categories contributed most to the change. For example, if total revenue increased by $10K MoM, how much came from Region A vs Region B vs Region C? Sum of segment-level changes equals total change. - By Factor (Mix vs Volume): In some cases, separate the change into factors like volume vs mix (or price vs volume, etc.)[8]. For instance, if average order value changed, part could be more orders (volume effect) and part because customers bought higher-priced items (mix effect). This is analogous to a Price-Volume-Mix (PVM) analysis in finance, which shows how much of a sales variance is due to price changes, volume changes, and product mix shifts[8][9]. Our tool will implement a generalized version when applicable.
Segment Variance (Additive): This is straightforward: for a chosen dimension (say “Product Category”), we compute metric in baseline period and metric in current period for each category. Then compute the difference per category. We can then sort these contributions to show top positive contributors and top negative contributors (if any). For example, a segment contribution table might show Category A: +$5K (meaning A’s increase accounts for 50% of total +$10K change), Category B: +$3K, Category C: -$1K (decline), etc. The “share of change” can be computed as each segment’s change divided by total change[10], though we must be careful if total change is near zero or negative (that can make percentage shares misleading). Typically, we would present absolute contributions and maybe percentage-point contributions (especially for rate metrics).
Mix/Factor Variance (Multiplicative or Average metrics): If the KPI is a product of components (like Revenue = Price * Volume, or an average = weighted sum / volume), a more nuanced attribution is needed. A classic method: hold one factor constant and see effect of changing the other, then vice versa: - Example: Revenue variance = Price effect + Volume effect + Mix effect[9]. If we have product-level data, mix effect can be computed by holding total volume constant but changing the composition of sales across products. In general, for domain neutrality, the server might not intrinsically know to do a PVM unless instructed, but we can include a generic algorithm: - Compute overall change Δ = Actual - Baseline. - For each product (or dimension member), compute differences in volume and price, etc. Summing up appropriately gives contributions (this is complex; often done via a waterfall computation in finance). - Alternatively, a simpler share-of-change for mix: If a category’s share of total changed from 20% to 30%, that 10% point increase in share contributed to the mix effect of the overall metric. However, without clear definition of “mix effect” in a general sense, we will limit to simpler cases in V1.
For initial implementation, the segment-additive approach will cover most needs (it identifies key drivers by category, which is highly useful in BI). The LLM can interpret these: e.g. “Product A contributed +5K to the sales increase (50% of the increase) while Product C’s decline offset -1K of it.”
Algorithm (Segment Contribution pseudocode):

# assume we have two time periods or two scenarios
baseline_data = filter(rows where Date in "previous period")
current_data = filter(rows where Date in "current period")
# Or if comparing actual vs target: baseline_data = target, current_data = actual
metric = "Sales"
dimension = "Category"
# First, aggregate metric by Category for each period:
baseline_sum = {} ; current_sum = {}
for each row in baseline_data:
    cat = row[Category]; val = parse_number(row[metric])
    baseline_sum[cat] += val
for each row in current_data:
    cat = row[Category]; val = parse_number(row[metric])
    current_sum[cat] += val
# Now align categories:
all_cats = union(keys(baseline_sum), keys(current_sum))
contributions = []
for cat in all_cats:
    base_val = baseline_sum.get(cat, 0)
    curr_val = current_sum.get(cat, 0)
    diff = curr_val - base_val
    contributions.append((cat, diff))
total_change = sum(diff for diff in contributions)
# Optionally compute share of change:
for (cat, diff) in contributions:
    percent_of_change = (diff / total_change)*100 if total_change != 0 else None
    output(cat, diff, percent_of_change)
# Then sort contributions by diff descending
This can be done in streaming by reading the file twice (once for baseline filter, once for current filter) or in one pass if the data is sorted by date (we can accumulate in one pass by checking the date and adding to baseline or current accordingly; streaming iterators allow sequential access, so likely two passes is simpler unless data is interleaved). The complexity is O(N) for filtering and summing. With large N, two passes are acceptable under streaming constraints if N is in the hundreds of thousands or a few million, especially if we limit the number of groups stored in memory (categories). If a dimension has extremely many categories, this step might slow or exhaust memory; as mentioned, we can mitigate by focusing on top changes: use a min-heap of size N_max to keep the largest contributions as we compute.
Assumptions & Edge Cases: We assume the user or planning logic specifies what comparison to do (e.g. “compare last month vs prior month for KPI X”). If not specified, the tool might automatically choose the most recent period vs previous (e.g. last full week vs week before). If a target is identified and the user wants variance vs target, baseline_data would correspond to target values (which might be provided in a separate column or separate sheet – the tool must support that by either aligning by keys or simply treating target as another series). The algorithm above also assumes additive metrics; if metric is a percentage or ratio, segment “contribution” to difference is trickier (the difference is not purely additive by subgroups due to weighting). In such cases, we might refrain from attributing mix without explicit instruction.
False positives: A potential pitfall is attributing meaning to a difference that is within normal variation or noise. Our tool will not by itself judge statistical significance of the change (unless we incorporate an anomaly test); it will just report the changes. The LLM client, however, can decide to highlight only large contributions. We also note if total_change is near zero, the percentage of change becomes unstable (huge percent contributions for small absolute differences); the insight card will focus on absolute changes in that scenario and perhaps caution that total change is minimal.
5. Composition & Mix Shift Analysis
Definition: Analyze the composition of a total metric across categories and how that composition changes over time or relative to a baseline. For example, “Product category A accounted for 30% of sales this quarter, up from 20% last quarter” – that is a mix shift insight. Composition is usually expressed as percentage shares of a whole, and mix shift refers to changes in those shares. This is related to variance attribution but specifically focuses on relative share changes which might indicate an underlying trend (e.g. dependency on a few items increasing).
When it’s meaningful: Share-of-total is useful when the categories sum to a meaningful whole (e.g. market share, portfolio composition). It can be misleading if the total itself changes drastically or if new categories appear/disappear. The tool uses composition analysis primarily to flag cases like: - A category becoming disproportionately dominant or shrinking, as measured by Herfindahl-Hirschman Index (HHI) or top-N share (see next section on concentration). - Significant shifts in share: e.g. one segment’s share of revenue grew from 10% to 25%, which might warrant attention even if absolute change is moderate.
Algorithm: For a given dimension and period, compute each category’s percentage of total (simply category_sum / total_sum). Do this for two periods if comparing. Then compute share change per category. For example:
for each category:
    share_current = current_sum[cat] / total_current
    share_prior = prior_sum[cat] / total_prior
    share_diff = share_current - share_prior
We then perhaps rank by share_diff to see which categories gained or lost share. The insight could be phrased as “[Category X] now constitutes 25% of the total, up from 15%, gaining +10 percentage points of mix.” The share-of-change metric is another angle: this is each category’s contribution to the total change (which we did above). We will be careful to distinguish these: - Share of total vs Share of change: Share of total is composition at a point in time; share of change tells how much of the growth or decline each part accounted for[10]. Both can be reported. For example, “Category A contributed 50% of the revenue growth” (share of change) versus “Category A now makes up 30% of total revenue” (composition).
Assumptions: Mix shift is most interpretable when comparing proportional data. If total baseline is very different from total current, a category can increase share even while declining in absolute terms (if total falls faster than that category), so one must be cautious. The tool will have the LLM include context like “(Total fell overall, so some categories’ share rose despite revenue drop).” The calculations themselves are straightforward and low-cost.
False positives & Mitigation: A small change in share might not be important if it’s within normal fluctuation. We might incorporate a threshold (e.g. only mention share changes above say 2 percentage points, or some multiple of expected variability if known). In domain-neutral context, we set a conservative threshold or allow the LLM to decide which changes are noteworthy. Another edge case: new categories with zero in baseline have undefined percentage growth or share change – the tool will highlight that as a special case (“new category introduced accounting for X% of current total”).
6. Concentration & Dependency Risk
Definition: Measure how concentrated the values are among top categories or individuals. This uses metrics like Top-N share (e.g. top 3 products = 80% of sales) and the Herfindahl-Hirschman Index (HHI) which is the sum of squared share fractions[11][12]. These indicators help identify if the dataset is heavily dominated by a few entries (which could signal risk – e.g. revenue relying on one client).
General Utility: In many domains, understanding concentration is useful: - In sales, you might check if a few customers or products account for most of revenue (Pareto 80/20 principle where ~20% of causes drive 80% of outcomes[13]). - In operations, maybe a few issue types cause majority of delays, etc. - In finance, a portfolio highly concentrated in one asset is riskier.
As a neutral tool, we define broad thresholds: For example, using the HHI interpretation from economics, an HHI above 0.25 (25% in fractional form, or 2500 in the 0–10000 scale) is considered “highly concentrated”[4]. Moderately concentrated is 0.15–0.25[4]. We won’t assume those thresholds apply exactly (since they come from antitrust contexts of market share), but they provide a reference. We can use Top-N share as a more intuitive measure: e.g. “Top 1 category = 50% (very high concentration), Top 3 = 90% suggests heavy reliance on a few segments.”
Algorithm: Sort categories by their value (e.g. descending by revenue). Compute cumulative share. Compute HHI as sum((value/total)^2). For example:
values = list of all category totals (e.g. sales by product)
total = sum(values)
shares = [v/total for v in values]
shares.sort(descending)
top1 = shares[0], top3 = sum(shares[0:3]), etc.
HHI = sum(s^2 for s in shares)
This is trivial to compute once we have an aggregation by category (which we likely do from earlier steps). Streaming to get HHI: yes, just need sums of squares of shares which means we need all or top categories anyway. In extreme large categories count, computing HHI exactly requires iterating through all groups after aggregation (which we will have done if grouping); if grouping is too large, we might approximate by sampling tail categories (though small categories contribute little to HHI since squared small share is tiny, so truncating small ones has minimal effect).
Interpreting for Insight: We will have the planning logic include a step: “Assess concentration of [measure] across [dimension]”. If, say, HHI corresponds to high concentration or top1 > 50%, the tool will generate an Insight Card like “Concentration Risk: The largest region accounts for 55% of total sales, indicating a potential dependency risk (HHI = 0.35, highly concentrated)[4].” If concentration is low (e.g. many categories each ~ equal share), the insight might simply note “No single segment dominates (no major concentration issues).” We can define generic language: - Unconcentrated (HHI < 0.15): “very diversified across segments.” - Moderate (0.15–0.25): “moderately concentrated – watch top contributors.” - High (>0.25): “highly concentrated – a few segments dominate.”
False positives: Using these generic thresholds on any data might not always make sense (e.g. if categories are not of equal importance inherently, but that’s a deeper domain question). However, as a neutral heuristic it’s fine. We will be careful not to frame concentration as “bad” universally; just state it as a characteristic. Another risk is if there is an “Other” category grouping many small ones, it could mask true fragmentation – but since we are the ones who might group others, we will account for that in how we compute (if we group long tail into “Other”, we will treat “Other” as a category in concentration calc; that might understate real concentration among named categories, but it’s acceptable as a representation).
7. Exception & Outlier Detection
Definition: Identify notable anomalies or exceptions in the data – data points that deviate significantly from the norm. This could be applied to: - Time series: detect if a particular period’s value is abnormally high/low compared to historical trend. - Categories: find if a particular segment’s value is an outlier compared to peers. - Any distribution: highlight values that are out of expected range (e.g. negative values where usually positive, or zeros where usually non-zero, etc., which overlaps with data quality).
Our approach favors conservative statistical methods to avoid false alarms. We will use robust measures like the modified z-score with median and MAD (Median Absolute Deviation). Specifically, a common rule is flagging points with modified z-score > 3.5 in absolute value (per Iglewicz & Hoaglin’s recommendation)[14]. This method is robust against distortion by outliers since it uses median and MAD[15][16].
Algorithm (example for one column distribution):

# Compute median and MAD in one pass (if streaming, we might need two-pass or approximation because median needs data sorted)
values = []
for each row:
    if row[col] is not null:
         values.append(row[col])
median_val = median(values)
MAD = median(|val - median_val| for val in values)
threshold = 3.5  # modified z-score threshold
outliers = []
for each val in values:
    mz = 0.6745 * (val - median_val) / (MAD if MAD != 0 else 1e-9)
    if mz > 3.5 or mz < -3.5:
        outliers.append(val)
This yields which values are outliers. If we want to detect outlier rows, we might just identify the value and its context (e.g. “Week 32 had an unusually high value 500, which is an outlier by statistical test”). We can also use simpler z-score (mean and standard deviation) if data size is small or if we suspect normal distribution, but MAD is safer for arbitrary data[16].
For categorical comparisons, we could apply a similar idea on the group totals: e.g. compute the distribution of values across categories, and flag if one category’s value is far above the mean of others by some standard (though categories often have power-law distributions, so maybe check if top1 is an outlier relative to others—this is akin to concentration).
Streaming considerations: Finding a true median in a single pass streaming is non-trivial without reservoir sampling or approximate quantiles. However, given that outlier detection doesn’t need exact median to be effective, we can approximate: - Use an online estimator or P^2 algorithm for quantiles to estimate median and MAD. Or, - Do a first pass to gather distribution stats (maybe a histogram or sample) then compute median, then second pass to flag outliers. Since this is likely done on one column at a time, even two passes is okay if needed because it’s linear and probably not too memory heavy to store one column’s values if under certain limit (we can cap at say 100k values by sampling if dataset is huge).
Alternatively, a simpler conservative approach: use interquartile range (IQR) – mark anything beyond 3*IQR from Q1 or Q3 as outlier. This is easier to compute with partial histograms. The choice of method can be configured; for now, using modified z-score threshold ±3.5 is a clear standard[14].
Avoiding false positives: We will include logic to avoid labeling too many points as outliers: - If the data size is small (say < 10), we might not flag anything because statistical detection is unreliable. - We cap the number of outliers reported – e.g. at most top 5 outlier points – to avoid spamming. If more than a few points exceed threshold, it might indicate high variance overall rather than isolated anomalies (so we might treat it as no clear outliers because the distribution itself is wide). - If data is trending (e.g. steadily increasing), the latest point might be higher than mean and trigger a z-score but is not really an anomaly in context of trend. Our detection is univariate static; to handle trends, we might incorporate a check like detrending or requiring a very large jump from previous point. However, domain-neutral tool will keep it simple: it might flag it but the planning LLM can reason “the latest value is high but part of an upward trend, so perhaps not anomalous”. For now, we assume mostly stationary context for outlier flag.
Usage: The insight cards for exceptions will say e.g. “Exception: [Metric] in [Week 10] is 3.8σ above typical, suggesting an anomaly (X = 120 vs median ~ 50)[14].” Or for category: “One outlier store with sales far above others (Store 101: $5M vs median ~$1M).” The tool will ensure to phrase cautiously (“potential outlier”) unless absolutely certain.
8. Data Quality Checks
Definition: General sanity checks on the dataset that are not tied to business insight but to the integrity of data. These include: - Missing Data: Check for high fraction of missing or null values in columns, or entire empty columns. For example, if 20% of records have no value for “Revenue”, flag that the data has gaps. We can set a rule-of-thumb threshold (e.g. if >5% missing in a critical column, mention it). - Duplicates: If an ID column was inferred (e.g. a primary key like “OrderID”) but duplicates exist, that’s a potential data issue (unless duplicates are expected). We can check if any duplicate keys or duplicate rows. - Range and Sign Checks: Identify values that fall outside expected ranges: - Negative values in fields that should be non-negative (e.g. negative quantity or negative price, which could indicate returns or errors). - Percentages over 100% or below 0% (if a column looks like percentage but is out of bounds). - Dates that are in the future or far past unreasonable bounds (e.g. year 2099 or 1900 in a dataset of current business metrics). - Zero values in contexts where zeros are unexpected (like if every record should have an ID but some have 0 or blank).
•	Type consistency: If a column is mostly numeric but some entries are non-numeric strings, that indicates mixed data types (maybe a data import issue for those rows). Similarly, if date column has some non-date text.
Algorithm: These checks are simple and can be done in one streaming pass:
stats = {col: {null_count:0, total_count:0, negative_count:0, >100_count:0, non_numeric_count:0, ...} }
dup_check = {}  # for ID duplicates if any ID column
for each row:
  for each col:
    stats[col].total_count += 1
    if row[col] is None or row[col] == "":
        stats[col].null_count += 1
    else:
        if is_number_col(col) or inferred_type[col] == "Measure":
            val = parse_number(row[col])
            if val is None: stats[col].non_numeric_count += 1
            else:
                if val < 0: stats[col].negative_count += 1
                if (looks like percentage col) and val > 1: stats[col].over1_count += 1
        if inferred_type[col] == "ID":
            id_val = row[col]
            if id_val in dup_check: dup_check[id_val] += 1 else dup_check[id_val] = 1
After loop, scan stats for anomalies: e.g. if null_count/total_count > 0.05, that’s notable missingness. If negative_count > 0 for an unsigned metric (the tool might guess which metrics should be ≥0 by name or by seeing if any negative at all – if only a couple negatives, maybe they are returns, but we’ll still flag them gently). For IDs, check any dup_check entries >1. We won’t do a full duplicate row check unless needed, but duplicate IDs suffice.
Output: Insight cards or warnings such as: - “Data Quality: 10% of entries in Discount are missing, which may affect analysis completeness.” - “Data Quality: Found 3 duplicate Customer IDs (likely data entry issue).” - “Data Quality: Negative values detected in Price (possibly refunds or errors).” - “Data Quality: Inconsistent data type in Date column – some values are not valid dates.”
We keep these messages neutral and factual, possibly in an “Assumptions/Quality” section of the output so that the user/LLM can decide to correct or proceed with caution. Low compute cost: these are O(N) or less.
Assumptions: We assume the first row might be headers (Excel), etc. The server likely will know to skip header row in streaming. Mixed types detection will rely on parse failure counts. We might define a list of units or clues (if column name has “%” or “rate”, treat >1 as >100% if values are fraction vs percent format confusion).
False positives: We will avoid over-reporting. E.g., a small number of negatives in a generally positive field might be legitimate (e.g. returns as negative sales). The insight will simply note it as an observation, not necessarily “error”. The LLM can use that info or ignore if context explains it. We set conservative triggers (e.g. if just 1 negative out of 1000, maybe that’s fine – or we still mention “one negative value found”). The user can decide its relevance.
 
Each of the above primitives is designed for streaming: they require either single-pass or a controlled multi-pass (which is manageable for moderate data). The server will implement them in Go, likely leveraging Excelize’s streaming row reader for iterating through an XLSX sheet row by row, and only keeping necessary aggregates in memory. For instance, Excelize allows reading rows in streaming mode without loading the entire file, enabling processing of very large sheets with constant memory[17][18]. Our algorithms align with that: e.g. summing or counting on the fly, grouping with dictionary accumulation (memory proportional to number of groups, not rows). Where needed (like median calc), we can do partial data structures or limited buffering. These primitives thus provide deterministic, bounded operations – no LLM involvement, no unpredictable latency beyond linear scan – which the LLM can invoke safely as tools. Table 1 below summarizes the primitives, their outputs, complexity, and built-in safeguards:
Table 1: Insight Primitives Summary (for brevity in text, not all details included)
Primitive	Output example	Complexity	Built-in Caps / Safeguards
Role Inference	{Column: "Date" -> Time, "Revenue" -> Measure, ...}	O(N) for sampling	Max 100 rows sampled per col; uncertain classifications yield questions.
Time Grain Detection	"Grain": "Monthly", "Comparison": "MoM and YoY"	O(N)	Requires ≥2 periods for comparison; if ambiguous, ask user.
Aggregation (Group By)	e.g. {Region: [ (North, 500), (South, 300), ...]}	O(N)	If groups > 1000, only top groups retained (others combined as "Other").
Variance Attribution	e.g. {TotalChange: +10K, by Category: [A:+5K, B:+3K...]}	O(N) (2N for two periods)	Skips if baseline or current data missing; very small total change triggers cautious phrasing.
Composition & Mix Shift	e.g. Category A: 30% (was 20%) -> +10pp	O(N)	Only highlights share changes beyond threshold (e.g. ±5pp).
Concentration Metrics	e.g. Top3 Share: 85%, HHI: 0.30 (high)	O(G) where G=groups	If grouping large, might approximate tail; uses known thresholds for context[4].

Outlier Detection	e.g. Outlier: Week 5 value 120 (3.8σ above norm)	O(N) (with 2 passes possible)	Max 5 outliers reported; needs ≥5 points to consider; uses robust z-score[14].

Data Quality Checks	e.g. 5% IDs duplicate, 2% values missing	O(N)	Only report issues above minimal thresholds; non-critical issues marked as warnings.
By providing these primitives, the server gives the LLM building blocks to answer typical BI questions (trends, key drivers, anomalies) in a controlled manner. Next, we discuss how the Sequential Insights planning schema orchestrates these primitives in an interactive loop.
Planning Schema & Sequential Workflow
The Sequential Insights planning schema defines how the server and LLM coordinate a multi-step analysis. Instead of a single monolithic “insight” operation, we implement a planning tool that helps break the analysis into sequential steps – much like the MCP Sequential Thinking server but tailored to data insights. The schema ensures domain-neutrality: it doesn’t hardcode any business logic, just the structure to pose questions and recommend tools based on the data context identified so far.
Input Schema for sequential_insights
When the LLM invokes the planning tool, it supplies an input JSON with fields such as: - objective (string, required): The high-level analysis goal or question. For example: "objective": "Analyze the sales performance and drivers for the latest quarter". This gives context to guide which primitives to use.
- hints (array of strings, optional): Any prior knowledge or user-provided hints about the data. This might include known KPI names or domain info (e.g. ["KPI=Revenue", "Baseline=last year", "Target column=Plan"]). Hints can bias role inference or tell the tool what to focus on (without hardcoding domain rules).
- constraints (object, optional): Any limits or preferences for the analysis, such as time ranges ("time_range": "2023 Q1 to Q4"), maximum categories ("max_groups": 10), or performance directives ("mode": "fast_approx" if we had such toggles). Constraints ensure the server doesn’t overrun resource limits or can degrade gracefully (for instance, if max_rows: 1e6, the server might sample beyond that).
- cursor (string, optional): A reference to the data source (Excel file path or sheet pointer). Since our API is path-based (no workbook IDs), the client must provide the file path or URI for the data. If a cursor (like an open file handle) is available from a previous step, it can be provided for efficiency. The server will use cursor with precedence over a raw path if both are given (ensuring continuity) – this aligns with the “path-only, optional cursor with precedence” design. For example: "cursor": "file://current_workbook.xlsx/Sheet1".
This input schema is lightweight and domain-agnostic. An example input might be:

{
  "objective": "Explain the change in profit from last quarter to this quarter",
  "hints": ["time_grain=quarterly", "target=Profit_Target"],
  "constraints": {"max_groups": 20},
  "cursor": "file://FinanceReport.xlsx/Q4"
}
Here, the tool knows the user cares about profit change QoQ, has a target to compare, and not to overwhelm with more than 20 segments in any breakdown.
Output Schema for Planning (InsightPlan)
The planning tool returns a JSON object guiding the next steps. The schema includes: - current_step (object): A description of the current analysis step or finding. It often contains: - step_description (string): A brief note of what the step is doing (or the finding it produced). E.g. "step_description": "Identified main measure 'Sales' and time dimension 'Date'; recommending trend analysis." - insight_cards (array, optional): If the step already yields a concrete insight, it can be packaged here. Each element would be a structured insight (discussed below in Insight Cards Guide). Often the planning steps will accumulate findings here. - recommended_tools (array): A list of next tool calls that the LLM should consider, with rationale. Each entry can have: - tool_name (string): e.g. "aggregate_by_time" or "group_by_dimension". - parameters (object): suggested parameters for that tool (e.g. which columns to use). - confidence (number): a score 0–1 indicating how strongly the server recommends this tool[2]. - rationale (string): explanation of why this tool is appropriate[2]. - priority (integer): suggested order if multiple tools[19]. - questions (array, optional): Clarifying questions for the user or LLM before proceeding. For instance, if ambiguity remains (“Multiple date columns found”), the server might include: "questions": ["Which date field represents the timeline of interest (OrderDate or ShipDate)?"]. These are surfaced so the LLM can ask the user or decide itself.
•	next_step_conditions (array, optional): Criteria or checks for moving to the next step[20]. For example: ["If a significant trend is found, proceed to variance analysis.", "If no trend, consider segmentation instead."]. This helps the LLM decide the branch. (This is similar to next_step_conditions in the sequential thinking server[20].)
•	thought_progress (object): This can include step_number (int) and total_steps (int) if the server has an idea of an overall plan length[21]. For example, step 1 of 5. In many cases, the total steps may not be predetermined (the process is dynamic), so total_steps could be omitted or a rough guess.
•	meta (object, optional): Additional metadata about the session or data, such as:
•	data_summary: e.g. number of rows processed, number of columns detected, processing time.
•	assumptions: list of assumptions the tool is making (like “assuming financial year = calendar year” if it had to guess).
•	cursor or session_id: if the server wants to maintain a handle to the data for subsequent calls (though we avoid stateful IDs, but maybe it can echo back the cursor for convenience).
•	warnings: any warnings (e.g. “Data truncated to first 1e6 rows due to limit”).
An illustrative output JSON (with some filled fields) could be:

{
  "current_step": {
    "step_description": "Time series trend detection for 'Sales'",
    "insight_cards": [
       {
         "type": "ChangeOverTime",
         "finding": "Sales have grown steadily over the past 12 months, currently 15% higher YoY.",
         "impact": "Growth accelerated in Q4 (5% QoQ increase).",
         "evidence": "Total Sales this quarter were $1.05M vs $1.00M last quarter[3].",
         "assumptions": ["Assumes consistent seasonality year-over-year"],
         "next_action": "Recommend investigating which regions contributed most to this growth."
       }
    ],
    "recommended_tools": [
       {
         "tool_name": "group_by_dimension",
         "parameters": {"dimension": "Region", "measure": "Sales", "aggregation": "sum", "period": "current_vs_prior"},
         "confidence": 0.9,
         "rationale": "Identify top regions driving the Sales increase",
         "priority": 1
       },
       {
         "tool_name": "outlier_detection",
         "parameters": {"column": "Sales", "by": "week"},
         "confidence": 0.6,
         "rationale": "Check for any anomaly in weekly sales trend",
         "priority": 2
       }
    ]
  },
  "next_step_conditions": [
    "If regional breakdown shows one region surging, drill down further into that region.",
    "If growth is uniform, consider product category analysis next."
  ],
  "thought_progress": { "step_number": 2, "total_steps": 5 },
  "meta": {
    "data_summary": {"rows": 50000, "columns": 10},
    "warnings": ["Profit margin column mostly empty, excluded from analysis."]
  }
}
This structure gives the LLM a clear roadmap: it sees that the current step found a YoY growth insight (with an Insight Card drafted), and it has tool suggestions to proceed (e.g. do a regional breakdown). The LLM can then choose to execute the group_by_dimension tool next to produce a Region breakdown, as suggested.
Importantly, the schema is flexible: if earlier steps are still about clarifying context, the insight_cards might be empty and questions populated instead (e.g. first step might be just asking the user which metric is primary if unclear). As the process continues, insight_cards accumulate findings which the LLM can present in the final answer. The separation of recommended_tools and questions ensures the LLM knows whether to gather more info or to proceed with analysis.
Example Sequential Scenarios
To cement the understanding, here are three example scenarios with how the planning schema would guide them:
Scenario 1: Trend Check (Generic Business KPI)
Objective: “Identify any significant trends in our weekly sales data.”
- Step 1 (Plan): The tool infers roles and finds a date column with weekly data, and a sales measure. It returns: - step_description: “Detected Time dimension = Week, Measure = Sales. Data is weekly. Suggesting WoW trend analysis.” - recommended_tools: [ {tool: "aggregate_by_time", params: {"freq":"week","metric":"Sales"}, ...} ] (confidence 0.95, rationale: "Compute sales per week to see trend"). Possibly also an outlier_detection suggestion. - (No questions, as this is straightforward.) - Step 2 (After tool execution): Suppose the aggregate_by_time tool returns a time series. The planning tool now analyzes it, finds an upward trend and a seasonal spike. It outputs: - insight_cards: e.g. a ChangeOverTime card: "Weekly Sales have an upward trend, increasing ~2% WoW on average. A notable spike occurred in Week 35 (holiday sale)[14]." - recommended_tools: [ {tool: "variance_to_baseline", params: {"current_period":"latest week","baseline_period":"prior week"}} ] (to quantify the latest change), and perhaps a suggestion to check by product or region if relevant. - questions: none, unless something unclear. - The LLM now has a trend insight and might proceed to quantify last week’s jump and then conclude. Alternatively, if the spike is unusual, it might call outlier_detection explicitly as recommended to confirm it’s an anomaly beyond normal range (the planning could already incorporate that if coded).
Scenario 2: Variance vs Baseline
Objective: “Explain why profit is down compared to last year’s quarter.”
- Step 1: Tool identifies time grain quarterly, measure Profit, possibly finds a “Profit_Target” column (if hint given or header matched). It might output: - step_description: “Identified Profit as KPI. Comparing Q4 2025 to Q4 2024 (YoY) since annual seasonality matters[22].” - recommended_tools: [ {tool: "aggregate_by_time", params: {"freq":"quarter","metric":"Profit","periods":["2024 Q4","2025 Q4"]}} ] (to get the values to compare). Maybe also target_compare if a target exists. - questions: If there were multiple profit measures or ambiguity, it might ask “Use Operating Profit or Net Profit?” for clarification. - Step 2: After getting totals, planning sees profit fell say -5%. It now wants to attribute it. It outputs: - insight_cards: a Variance card: "Profit in Q4 2025 was $95M, down $5M (-5%) from $100M in Q4 2024." (finding & impact). - recommended_tools: [ {tool: "group_by_dimension", params: {"dimension":"Region","measure":"Profit","compare":["2024 Q4","2025 Q4"]}}, {tool: "group_by_dimension", params: {"dimension":"ProductLine","measure":"Profit","compare":[...]} } ] with rationales like: "Check which regions or product lines saw profit declines." - Possibly a question: “Any specific dimension (region, product, etc.) you suspect drove the change?” – if hints or context from user. - Step 3: Suppose the user/LLM chooses region breakdown. The group_by_dimension returns each region’s profit for both years. Planning finds Region X dropped significantly. It outputs: - An Insight Card: "Region X profit dropped by $4M (–20%), contributing ~80% of the total profit decline." - Maybe another card if another region offset gains. - Next recommended step: perhaps drill into Region X further by product or cost analysis. - And so on, until the chain yields a set of insights explaining the variance.
Scenario 3: Segmentation Analysis
Objective: “Find key differences across customer segments in Q1.” (No explicit metric given – maybe multiple measures). - Step 1: Role inference finds maybe “Customer Segment” as a categorical dimension, and several measures (Revenue, SatisfactionScore, etc.). If multiple measures, it might ask the user which to focus on: - questions: ["Which metric do you want to compare across segments? (e.g. Revenue, Satisfaction)"]. - The LLM/user picks Revenue. - Step 2: Planning now knows: compare Revenue by Segment. It outputs: - recommended_tools: [ {tool: "group_by_dimension", params: {"dimension":"CustomerSegment","measure":"Revenue"}} ] with rationale "Compute revenue per segment to identify highest/lowest". - Possibly also suggest a variance if multiple time periods: if Q1 implies a timeframe, maybe add "compare to last Q1". - Step 3: After execution, suppose it finds 3 segments with revenues and one segment much higher than others. Planning outputs: - Insight Card: "Segment A contributes 60% of Q1 revenue, far above other segments (next highest is 20%)." (this highlights concentration). - It might also calculate HHI or mention 60% is a concentration sign. - Recommended next: maybe check why Segment A is high – suggest further breakdown within that segment (if data has sub-categories) or perhaps check if profitability aligns or if this segment also has highest growth. - Step 4: If growth or target is relevant, it could branch. But if the objective was just differences, the above might suffice as the main finding (Segment A dominates, etc.), possibly supplemented with a stat like average order size differences or satisfaction differences if available (the tool could recommend comparing another measure across segments for a multi-dimensional insight).
These examples show how the planning schema orchestrates a logical progression: clarify context -> analyze overall -> drill down -> highlight insights -> suggest next steps, much like an analyst’s approach but in a structured, JSON-guided form. The server’s role here is to provide the scaffolding (as above) and sometimes to carry out simple analysis in the planning (like noticing which region had the largest drop, since it has the data).
Client vs Server Responsibilities (RACI Matrix)
Clarity on what the server handles vs what the LLM client handles is crucial for a robust system. Below we delineate responsibilities, roughly mapping to a RACI (Responsible, Accountable, Consulted, Informed) model, though we’ll describe in prose rather than a strict table:
•	Understanding User Intent: Client (LLM) is responsible for interpreting the user’s question and determining the analysis objective. The server is not aware of the user’s natural language query; it only sees structured requests. For example, if the user asks “Why did our sales drop?”, the LLM decides this means a variance analysis on sales, and it formulates a objective: "Explain sales drop vs previous period" input for the planning tool. The server is not accountable for any misinterpretation of the question – that falls to the LLM (with possible user clarifications).
•	Clarifying Ambiguities: Client (LLM) takes the lead in asking the user follow-ups, but the Server (Planning tool) will proactively surface potential ambiguities via the questions field. Essentially, the server consults the LLM on uncertainties by providing those questions. For instance, if multiple measure columns could be “sales”, the planning output will list that question; the LLM is then responsible for resolving it (by either deducing from context or asking the user). The client is accountable for getting clarity, the server is just providing prompts.
•	Data Access & Preparation: Server handles reading the Excel data via provided path or cursor. The LLM does not see raw data unless it explicitly uses a resources/read or similar (which might not be needed if the server tools summarize it). The server ensures only allowed paths (e.g. within an allow-list or not reading outside the workbook) – this is a security measure. Concurrency caps mean the server might reject or queue if too many requests; the LLM should handle such errors gracefully (perhaps by waiting or summarizing fewer things). The server is responsible for delivering correct data content for a given path and for not leaking anything beyond requested scope.
•	Computation of Insights: Server is responsible for all computations using the primitives. It guarantees determinism: e.g., summing sales, computing HHI, etc., and returning factual results with citations if needed (in our design, citations are internal references to data, not external web, so likely the server doesn’t do narrative citations; it just returns numbers or minimal text). The LLM is not doing math on its own (to avoid errors); it relies on server results. The server is also accountable for handling streaming so that large data doesn’t time out or crash – if data is too large, server might either sample or abort with an error indicating data too large. The LLM then can decide to sample or ask user to restrict the data.
•	Narrative & Insight Synthesis: Client (LLM) is responsible for turning the outputs (Insight Cards and data stats) into a coherent narrative or answer for the user. The server provides structured info and even suggested phrasings in cards, but those are templates – the final phrasing should be polished by LLM to match the user’s tone preferences. Also, the LLM is accountable for factual accuracy in the final answer, which means it should rely on server-provided numbers (it can quote them or refer to them) and not hallucinate new facts. If an insight seems contradictory or uncertain, the LLM should double-check via another tool call or clarify assumptions, rather than guessing. Essentially, the server informs the LLM with data; the LLM takes accountability for the explanation.
•	Error Handling: If the server returns an error (e.g. "Tool X failed: sheet not found" or "streaming limit exceeded"), the LLM must decide what to do. Likely, the LLM will either inform the user of the issue or try an alternative approach (e.g., ask the user to provide a smaller file or pick a specific sheet). The planning server can also have meta warnings as described; the LLM should incorporate those (for instance, if data quality check finds duplicates, the LLM might mention “(Note: data had duplicates, analysis assumes each record unique)” to maintain transparency). The LLM is Responsible for checking the meta field for any warnings or truncation notes, to avoid misleading interpretations. The server in turn is Accountable for populating those meta warnings when needed (like truncation due to too many categories or similar).
•	Tool Invocation Orchestration: Client (LLM) orchestrates which primitive tool to call and in what sequence, guided by the planning output. The planning tool on the server suggests a plan, but doesn’t autonomously call other tools (no server-side chain-of-tools to keep things deterministic). The LLM decides, e.g., “Alright, the plan recommends I run group_by_dimension on Region, I will do that now.” So the client is Responsible for tool chaining logic, whereas the server’s planning tool is Consulted for recommendations. There’s no hidden loop on server – it’s all visible to LLM which step it’s on.
•	Safety and Governance: The server ensures that only safe, allowed operations are executed: e.g., if certain columns contain PII and a tool is not allowed to output them, the server should enforce (through allow-list or schema validation). The LLM is responsible for not asking the server to do disallowed things in the first place (the client could have a list of tools and know their permissions, but the server ultimately rejects forbidden requests). In terms of output safety (like not making unreasonable claims about sensitive attributes), that’s mostly on the LLM’s narrative. For example, identifying a person from data or guessing confidential info is not something the server would do; it just gives numbers. The LLM must ensure compliance with policies when presenting insights (e.g., avoid demographic profiling if not appropriate, etc.). So the LLM is Accountable to user-facing safety, while server is Responsible for data-level security and compliance (no unauthorized file access, etc.).
In summary, the server acts as a calculation engine and planning advisor, and the LLM acts as the brain and voice of the analysis. The server does heavy lifting with guaranteed correctness and the LLM does reasoning and communication. This split ensures determinism and traceability: every number in the final answer can be traced to a server result (which could even be cited with a reference to the data if needed), and the steps taken can be audited via the sequence of tool calls.
We ensure the design avoids the server trying to be “smart” with the LLM’s job: for instance, no LLM chain-of-thought on the server, no natural language generation on server – just structured outputs. Conversely, the LLM is kept from doing unvalidated calculations by delegating to tools whenever possible.
Insight Cards Guide
The final output of the analysis to end-users will often be a series of Insight Cards, each highlighting a specific finding. Here we describe standard templates for various card types. These templates help maintain consistency and completeness in insights: each card includes the finding, its quantified impact, evidence from data, any assumptions or context, and a recommended next action if applicable. By adhering to these templates, we make insights concise, actionable, and self-explanatory, which is key for broad usability.
Each card will have a type to indicate what kind of insight it is, which also dictates its content structure. The main card types include:
•	Change Over Time Card: For trend or time-based changes.
•	Finding: A sentence describing the trend or change. E.g. “Sales increased in Q4, reaching an all-time high.”
•	Impact: The magnitude or significance of the change. “This was a 10% increase quarter-over-quarter, and 5% above the previous record.”
•	Evidence: A snippet with actual numbers/dates: “Sales were $5.5M in Q4 vs $5.0M in Q3[3].” – the citation could refer to data source or calculation.
•	Assumptions: Any caveats like “Assumes Q4 is a full quarter; December data was preliminary.” If no special assumptions, this can be omitted or a generic “Based on provided data”.
•	Next Action: “Next, investigate which products drove this growth.” – guiding further analysis or business action.
•	Variance to Baseline Card: Explains difference between two conditions (current vs target, or vs last period).
•	Finding: “Profit fell short of target by $2M.” or “Revenue grew by $1M vs last year.”
•	Impact: “This is a 5% shortfall against the plan.” or “Year-over-year growth of 8%.”
•	Evidence: “Actual Profit: $38M, Target: $40M[3].” or “2025 Q1: $10M vs 2024 Q1: $9M.”
•	Assumptions: “Target values as provided in Budget_2025 sheet.” or “No adjustments for inflation.”
•	Next Action: “See which regions or products account for the shortfall.”
•	Composition Shift Card: Highlights how the makeup of a total changed.
•	Finding: “Product A now makes up the largest share of revenue.”
•	Impact: “It accounts for 30% of total sales, up from 20% last year.”
•	Evidence: “Product A $3M of $10M total (was $2M of $10M)[9].”
•	Assumptions: “Product categorization unchanged year-over-year.”
•	Next Action: “Examine if Product A’s growth came from volume or price increases.”
•	Driver Ranking Card: Ranks contributors to a result (often comes from variance attribution).
•	Finding: “Region East was the top contributor to the sales increase.”
•	Impact: “East added +$500K, ~50% of the total growth.”
•	Evidence: “East: $2.0M → $2.5M, West: $1.8M → $2.0M[10].”
•	Assumptions: “Assumes no other confounding effects (each region’s growth evaluated independently).”
•	Next Action: “Focus on East’s strategies that led to this growth.”
•	Concentration Card: Points out heavy dependence or diversity.
•	Finding: “Sales are highly concentrated in the top 2 clients.”
•	Impact: “The top 2 clients account for 55% of revenue (HHI = 0.30, considered high concentration)[4].”
•	Evidence: “Client A 30%, Client B 25%, others 45%.”
•	Assumptions: “All clients considered; if some revenue uncategorized, not included.”
•	Next Action: “Consider strategies to diversify client base to mitigate risk.”
•	Exception/Outlier Card: Flags anomalies.
•	Finding: “Q2 had an unusually low conversion rate.”
•	Impact: “The conversion rate dropped to 1.5σ below the quarterly average.” (Or in simpler terms, “significantly below norm by statistical test”).
•	Evidence: “Q2 conversion 2.0%, vs median ~3.5%[14].”
•	Assumptions: “Assumes consistent tracking; external factors (e.g. site outage) not accounted.”
•	Next Action: “Investigate causes (e.g. was there a specific campaign issue in Q2?).”
•	Data Quality Card: Notes any data issues found.
•	Finding: “Data quality issues detected: missing values and duplicates.”
•	Impact: “5% of records had no region specified; 3 duplicate Order IDs found.”
•	Evidence: “Null Region in 50 out of 1000 rows; OrderID 123 appears 2 times.”
•	Assumptions: “Analysis treated duplicates as separate records.”
•	Next Action: “Recommend cleaning data and rerunning analysis for accuracy.”
Each card is designed to be a self-contained snippet that could be shown in a dashboard or report. The Finding is the headline, Impact quantifies it, Evidence backs it up with numbers (and possibly a reference to where those numbers come from, like a data source or calculation), Assumptions provide transparency, and Next Action keeps the analysis or decision-making moving forward. This structure aligns with best practices in analytical communication (similar to how an analyst might present findings in a slide deck with key point, data backup, and recommendation).
In the implementation, the server’s planning tool might output a preliminary version of these cards in JSON form. The LLM can then paraphrase or combine them as needed in the final answer to the user. However, since the user specifically asked for an Insight Cards Guide, presumably the intention is that even the LLM’s final answer may present the findings in a bullet or card-like format, which these templates facilitate.
One important guideline: keep the phrasing concise and factual. Avoid speculation in these cards; any hypothesis should be labeled as an assumption or next step to verify. The narrative style is informative, and if needed, the LLM can always soften or elaborate when producing user-facing text.
For example, suppose the planning results in two cards – a Change Over Time (trend) and a Driver Ranking – the LLM’s final message might list them as bullet points or in a short paragraph per insight, ensuring the content from these structured cards is preserved with sources for credibility. Since we maintain citation of sources (which in our context could be internal, but if needed, we’d cite the data or known thresholds as we did in this doc), the user can trust the insights are grounded in data.
Risks & Trade-offs
Designing this system involves balancing competing priorities. Below we address key risks and the trade-offs we considered:
•	False Positives vs False Negatives: A major risk in automated insights is surfacing spurious findings (false positives) that could mislead users. We mitigate this by using conservative statistical thresholds (e.g. 3.5 MAD for outliers[14], requiring substantive percentage-point changes for mix shift, etc.) and by including assumptions/uncertainty in the output. The trade-off is we might miss some subtle insights (false negatives) because we set the bar high to declare something noteworthy. For V1, this bias towards caution is acceptable – users prefer no insight to a wrong insight. As we get feedback, thresholds can be tuned or even made interactive (like Tableau’s GROOT research suggests allowing user to adjust insight generation thresholds[23][24]).
•	Performance vs Depth: We aim to keep computations streaming and fast, which sometimes means not doing extremely in-depth analysis. For instance, we do one-level variance attribution, but not multi-factor regression or advanced correlation mining in V1. This trade-off ensures responsiveness and deterministic resource use. If a user needs deeper analytics (like predictive or multivariate analysis), those might come in V2 with careful design. We consciously avoid anything that can blow up complexity (like exhaustive search for every possible insight) – instead, we focus on a known set of insight patterns. This keeps the system predictable. The downside is some complex insights (e.g. “this metric correlates with that metric over time”) might be out of scope initially. However, basic correlation or trend detection can be added in a controlled way in future if needed.
•	Data Volume Handling: The server processes large data via streaming, but if data is extremely large (millions of rows), even streaming aggregation could be slow or hit time limits. Our design imposes caps, like grouping only top N categories or scanning only the last K periods for trend if data is too long. The risk is we may ignore some data or truncate analysis, which could omit insights. We mitigate by clearly indicating when truncation happens (in meta.warnings). The user can then choose to refine the query (e.g. focus on a subset). The trade-off is between completeness and practical latency. We lean toward giving an answer quickly with partial data rather than timing out trying to read an entire massive file.
•	Ambiguity and Context: Domain-neutral means the tool doesn’t have domain-specific knowledge (like knowing “profit” is usually calculated field or “customer churn” is an outcome metric). This can lead to misinterpretation of fields or results. For example, the tool might highlight an “anomaly” that a domain expert knows is normal seasonal behavior. Or it might not realize that a 5% drop in a metric is critical in one context but noise in another. The LLM’s reasoning and possibly user’s input have to fill this gap. The risk is the AI could either overstate or understate findings without domain context. Our planning loop addresses this by prompting clarifying questions (which effectively gets context from the user or allows the LLM to incorporate its trained knowledge). We also provide next-step suggestions so the analysis can iterate and refine rather than making one-shot conclusions. The trade-off here is complexity: a fully domain-aware system would require embedding a lot of logic or training, which we avoid to keep it general. Instead, we accept that some back-and-forth may be needed to hone in on relevant insights for the user’s scenario.
•	User Trust and Explainability: Automated insights can sometimes seem like a “black box” to users[23]. Our design counters this by 1) providing evidence and numbers for each insight, 2) making assumptions explicit, and 3) enabling the user (through the LLM) to ask follow-up questions or drill down further. The planning schema’s transparency (showing recommended next steps, etc.) ensures the LLM can always explain why a certain insight was pursued[25]. A risk was that the server might do too much behind scenes; we mitigated by not hiding any step – every intermediate result is accessible if needed. The trade-off is possibly a more verbose interaction, but it’s a worthy trade for explainability.
•	Integration Complexity: Combining an MCP server with a client LLM means potential points of failure in communication. The server might output a plan the LLM misinterprets, or the LLM might invoke tools incorrectly. We manage this with JSON schemas (the server validates input types and formats to avoid garbage calls). Still, a risk is if the LLM doesn’t follow the plan or overrides it erroneously. We treat the plan as advice, not mandate; the LLM’s agent logic (like using a ReAct or Plan-and-Execute paradigm) should be tested thoroughly. The sequentialthinking-tools pattern[26] already offers a template for how LLM should iterate with tool recommendations, which reduces this risk. The trade-off is we rely on a fairly advanced prompt/agent design on the client side – simpler agents might not handle the JSON well. However, given MCP’s premise, we expect the client to be capable of JSON I/O.
•	Data Privacy: Since the server just reads local Excel and returns aggregate results, it doesn’t inherently leak data externally. But the LLM seeing the results might include them in a conversation. We must ensure that if any sensitive data is present, the LLM doesn’t share it inappropriately. In an enterprise, this would be controlled via user permissions and possibly by post-processing the insights (like masking client names if needed). While our design doesn’t explicitly include a privacy filter, it’s something to consider in deployment. The current assumption is the user asking for insight has access to the data, so it’s not a privacy violation to show it. But if there are internal policies (like don’t expose individual customer identities in a summary), the client app would need to enforce those (or the server can be configured with rules, e.g., only output top categories aggregated, not individual identifiers). This is a trade-off between insight granularity and confidentiality. We lean on the side of providing insight and trusting the usage context, but this can be adjusted per use-case.
In summary, we’ve prioritized determinism, precision, and caution over breadth and autonomy. The tool is not trying to answer every possible question or find “mystical” patterns; it sticks to reliable insight patterns known in BI. The planning loop with human/LLM in charge ensures flexibility – if something seems off, the LLM can deviate or double-check. This interactive aspect reduces risk compared to a fully automated insight generator that might output something incorrect without oversight. Essentially, the trade-off is doing it with the analyst (AI assistant) rather than for the analyst, which aligns with the MCP philosophy of a copilot rather than an oracle.
Prioritized Recommendations (V1 vs V2)
To scope the implementation, we identify what should be tackled in Version 1 (MVP) versus what can be postponed to Version 2 (enhancements). This prioritization is based on value vs complexity:
V1 Scope (Essential Features):
1. Core Primitives Implemented: Role inference, basic aggregations, time grain detection, period comparisons (WoW/MoM/YoY calculations), top-N contribution breakdown, outlier detection (with MAD), concentration metrics, and data quality checks. These cover the vast majority of straightforward insights. Each should be well-tested on varied datasets. 2. Planning Tool Basic Loop: Ability to take an objective and produce at least a simple plan with recommended next step. Likely focus on one path at a time (no complex branching) in V1. For example, plan might simply do: identify structure -> do trend or comparison -> do one breakdown -> output insights. Keep it linear to start. Ensure the JSON schema for input/output is solid and the LLM can parse it. 3. Typed Tool Schemas & Validation: Use MCP-Go’s JSON schema capabilities to strictly define inputs for each tool and the planning interface. This prevents misuse and also documents how to call them. E.g., a schema for group_by_dimension that requires a dimension string and measure string, etc. The planning tool’s schema will define objective etc., ensuring the LLM provides those. 4. Streaming & Performance Safeguards: Implement the iterators for reading Excel via Excelize such that even large files (say up to 100k rows or more) can be processed. Test memory usage. Also implement cut-offs: e.g., if a grouping results in >100 categories, maybe automatically create an “Other” category combining the smallest ones, and note it in meta (so LLM knows the output was truncated). This ensures V1 doesn’t choke on unbounded scenarios. 5. Basic Client Integration: On the client (LLM) side, ensure it can follow the server’s plan. Possibly use an agent loop like: - Call sequential_insights with objective. - Parse response, execute the first recommended tool. - Call sequential_insights again (maybe with an updated thought context) or maybe the planning tool is stateful enough to handle multi-step? (We might design it stateless where each call yields the next step only. Alternatively, the planning server could maintain session context via previous_steps it provided[27]. V1 might avoid too much state and rely on LLM to feed relevant info back each time.) - Continue until no next steps or until a certain depth, then finalize insights. This requires careful prompt engineering on LLM side; likely an adaptation of sequentialthinking-tools example, but feeding in our context. 6. Safety Checks: Ensure the server only accesses the allowed file path. Add basic checks to avoid execution of formulas/macros in Excel (Excelize by default doesn’t execute macros, so fine). Possibly sanitize outputs (if a text field has malicious content, though unlikely in this context). 7. Documentation & Examples: Provide clear documentation for users (or the prompt/LLM) on how to use the tool. Include examples like the ones above so that the LLM’s prompt library can use them to shape its chain-of-thought. This is important to accelerate adoption and avoid misuse.
These V1 features provide a functioning end-to-end system for common tasks like “What happened last quarter?” or “Any anomalies in this data?”. They intentionally avoid complex statistical analyses or heavy ML.
V2 (Future Enhancements):
1. Advanced Insight Types: Introduce correlation analysis (e.g. find if two measures in the sheet move together), key driver analysis (maybe stepwise regression or decision tree to explain a metric – though that can become domain-specific quickly). Also, predictive insights (like forecasting next period) could be considered, but with caution. 2. User Feedback Loop: Allow user to provide feedback on insights – e.g. thumbs up/down or “ignore this insight” – and incorporate that. This could connect with the planning server adjusting confidence or not repeating certain analysis. It’s akin to customizing or filtering insights, aligning with research like GROOT’s user-driven adjustments[28][29]. 3. Refinement of Planning Schema: In V2, the planning tool could handle branching more explicitly (like maintain multiple possible next steps and adjust if one path doesn’t yield interesting results). It could also integrate more of the sequentialthinking-tools features such as branch IDs or revision of steps[30], though in data analysis context, linear progression is usually fine. We might add a feature where if initial analysis yields nothing noteworthy (e.g. “everything is flat”), the planning tool can pivot to another angle (like instead of time trend, look for outliers or correlations). This requires more intelligent evaluation of “nothing found”, which could be based on thresholds (e.g. no segment had >X change triggers looking at maybe a smaller aggregation grain or another measure). 4. Visualization Hooks: In the future, the server could provide small chart images or sparkline as part of insight (since Excelize can generate charts or at least the data for charts). While the user’s question doesn’t explicitly ask for it, a common BI insight feature is to show a visual. We could have an embed_image field with a mini chart. This is lower priority and would need ensuring image generation is deterministic and fits in payload. 5. Integration with External Data/Prompts: Possibly integrate “prompts” feature of MCP for common analyses. E.g., have a stored prompt that the LLM can request like a “Time Series Analysis Plan” prompt that the server provides, which the LLM can use to structure its approach. This might standardize how tasks are executed (some of this overlaps with our planning tool’s function). It’s an idea if we want to separate static instructions from dynamic planning. 6. Scalability and Concurrency: For wider deployment, we’d improve how the server handles multiple requests at once (thread pooling or queue management). Possibly caching partial results: e.g. if the LLM asks for an aggregation and then asks for a slightly different aggregation on the same data, the server could reuse the data in memory (if we allowed session state). Currently, statelessness means re-reading file each time; V2 might allow ephemeral caching via a cursor id (though given “no workbook IDs” constraint, we might use file path as key and cache open file handle for short time). Must be careful with stale data (mtime validation – check file modified time; if changed, drop cache). This is an optimization trade-off: stateless is simpler and safer (V1 approach), stateful caching is faster but requires invalidation logic (could be added in V2 for performance if needed). 7. Enhanced Natural Language Explanations on Server: Not to embed an LLM, but we could have a repository of template phrases for common findings so that the insight cards come out more polished. For example, if HHI > 0.25, the server might directly phrase it as “highly concentrated” using the threshold reference[4], which we have essentially coded. We could extend that approach for other patterns: e.g., if an outlier is 4σ away, maybe flag as “extreme outlier”. This is minor since the LLM can handle phrasing, but if we want consistent wording or multi-language support at server level, template-based generation could be a feature.
In essence, V1 delivers the backbone: the must-have primitives and a functioning planning loop for core BI scenarios. V2 would iterate by adding sophistication (more insight types, more dynamic planning, possibly more automation in deciding what’s interesting). We intentionally keep LLM-driven tasks (like interpreting “interestingness”) mostly in the LLM domain for V1 to avoid over-engineering logic that the LLM can handle. Over time, as we gather which patterns are most useful or which require special handling, we can bake some of that into the server.
Finally, we will continuously gather user feedback on which insights are useful or not, and use that to refine thresholds or add new primitives. For example, if users often ask for “compare vs forecast” and our tool doesn’t directly handle forecast accuracy yet, we might add a primitive for forecast error analysis in V2. Or if users want seasonally adjusted trends, we might incorporate that.
By phasing development, we ensure a usable product early (V1), and a clear roadmap for improvements (V2), without trying to solve every problem at once.
References
The design and recommendations above are informed by both the specifics of the MCP framework and general best practices in automated analytics. Key sources and their contributions are listed below:
1.	Sequential Thinking MCP Tools (Scott Spence’s adaptation) – Demonstrated how an LLM can be guided to choose tools with rationale and confidence scores[31][2]. This influenced our planning schema (recommendations with priority and rationale). Source: spences10/mcp-sequentialthinking-tools GitHub repository (2023) – Shows JSON output format for tool recommendations in a sequential reasoning context.[2][32]
2.	Excelize Streaming Documentation – Provided patterns for streaming large Excel reads/writes in Go. Ensured our algorithms align with one-pass streaming without random access. For example, writing 100k+ rows with StreamWriter is shown to be feasible[17]. Source: Excelize Documentation – Streaming (Ri Xu, 2023) – Illustrates handling of large Excel data via streaming APIs.[17][18]
3.	Tableau Automated Insights (GROOT research) – Highlighted limitations of rigid heuristics and the need for user control[23][24]. This reinforced our decision to keep the LLM in the loop (for flexibility) and to include assumptions and user-adjustable steps. Source: Tableau Research Blog on GROOT (Vidya Setlur, 2024) – Discusses customizing automated insight generation and issues with black-box insights.[23][24]
4.	Statology on Modified Z-Score Outliers – Gave a concrete threshold for outlier detection: modified z-score beyond ±3.5 as per Iglewicz & Hoaglin[14]. We adopted this robust method to flag anomalies conservatively. Source: Statology article "What is a Modified Z-Score?" (2021) – Explains using median & MAD for outliers and the ±3.5 rule.[16][14]
5.	Wikipedia on Herfindahl-Hirschman Index – Provided context and threshold values for concentration measures[4]. We used the <0.15, 0.15–0.25, >0.25 categories (unconcentrated, moderate, high) as general guidelines in our concentration insights. Source: Wikipedia – Herfindahl–Hirschman Index – Describes HHI and standard threshold values used in competition law.[4][12]
6.	CFO Dive on Price-Volume-Mix Analysis – Confirmed the concept of decomposing variance into price, volume, mix components[8]. This supported our inclusion of mix shift analysis and understanding that mix effect = change in share of components[9]. Source: CFO Dive article by Dayton Kellenberger (2021) – Details how PVM (price-volume-mix) analysis explains revenue changes and defines each effect.[8][9]
7.	Tableau Help on Dimensions vs Measures – Reinforced how to identify categorical vs quantitative fields[5]. We mirrored this in our role inference logic (qualitative values -> dimensions, quantitative -> measures). Source: Tableau Desktop Help – "Dimensions and Measures" – Defines dimensions as qualitative (names, dates) and measures as quantitative values[5], which guided our classification rules.
8.	Vivainsights (Microsoft) on Date Frequency – Provided a simple approach to detect date frequency (daily, weekly, monthly) using day-of-week and counts[6]. We incorporated similar logic (with some modifications) for time grain detection. Source: Microsoft Viva Insights R Reference – identify_datefreq – Explains how to classify a series of dates as daily/weekly/monthly with examples of logic[6].[6][33]
Each of these references helped shape a portion of the solution, ensuring that our design stands on proven techniques and community knowledge rather than reinventing the wheel. By integrating these insights, we aim for a tool that is both innovative in workflow and solid in methodology.
 
[1] [2] [19] [20] [21] [25] [26] [27] [30] [31] [32] GitHub - spences10/mcp-sequentialthinking-tools: An adaptation of the MCP Sequential Thinking Server to guide tool usage. This server provides recommendations for which MCP tools would be most effective at each stage.
https://github.com/spences10/mcp-sequentialthinking-tools
[3] [4] [11] [12] Herfindahl–Hirschman index - Wikipedia
https://en.wikipedia.org/wiki/Herfindahl%E2%80%93Hirschman_index
[5] Dimensions and Measures, Blue and Green - Tableau
https://help.tableau.com/current/pro/desktop/en-us/datafields_typesandroles.htm
[6] [7] [33] Identify date frequency based on a series of dates — identify_datefreq • vivainsights
https://microsoft.github.io/vivainsights/reference/identify_datefreq.html
[8] [9] Using a price-volume-mix analysis to improve performance | CFO Dive
https://www.cfodive.com/news/price-volume-mix-performanceDayton-Kellenberger/596889/
[10] [PDF] Streetcars and Economic Development
https://ppms.trec.pdx.edu/media/project_files/Streetcars_and_Economic_Development.pdf
[13] Pareto principle - Wikipedia
https://en.wikipedia.org/wiki/Pareto_principle
[14] [15] [16] What is a Modified Z-Score? (Definition & Example)
https://www.statology.org/modified-z-score/
[17] [18] Streaming write · Excelize Document
https://xuri.me/excelize/en/stream.html
[22] Comparing week-over-week results - Splunk
https://www.splunk.com/en_us/blog/tips-and-tricks/comparing-week-over-week-results.html
[23] [24] [28] [29] Beyond the Default: Customizing Automated Data Insights with GROOT
https://www.tableau.com/blog/tableau-research-default-customizing-automated-data-insights-groot
