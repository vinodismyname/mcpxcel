# MCP Excel Analysis Server: Architecture and Implementation Plan

## Introduction

We propose a **Model Context Protocol (MCP)** server in Go that enables Large Language Models (LLMs) to read, analyze, and manipulate Excel spreadsheets without overloading the model’s context. This server will use the **Excelize** library for Excel file operations and the **mark3labs/mcp-go** framework for MCP integration. The design emphasizes _smart tool usage_, allowing LLM clients to retrieve only relevant data or summary information instead of entire sheets, thereby avoiding context window exhaustion[\[1\]](https://blog.langchain.com/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/#:~:text=When%20I%20first%20tried%20to,we%20get%20a%20decent%20result)[\[2\]](https://blog.langchain.com/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/#:~:text=,like%20Excel%20numeric%20date%20encoding). It will also be built with concurrency in mind (leveraging Go’s goroutines for parallel requests[\[3\]](https://gyliu513.medium.com/build-and-run-a-go-mcp-server-in-5mins-0ed28e592069#:~:text=,model%20context%20requests%20in%20parallel)) and a pluggable LLM interface supporting multiple providers (OpenAI, Anthropic Claude via AWS Bedrock, etc.). Below, we detail how the server architecture and tools address these goals, followed by a step-by-step implementation (PR) plan.

## Avoiding Context Overload with Smart Tools

Reading a large spreadsheet directly into an LLM prompt is infeasible – large tables can exceed token limits and even cause errors[\[1\]](https://blog.langchain.com/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/#:~:text=When%20I%20first%20tried%20to,we%20get%20a%20decent%20result). For example, simply chunking an entire sheet into text often produces incomplete or incorrect summaries and can overwhelm the context window[\[2\]](https://blog.langchain.com/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/#:~:text=,like%20Excel%20numeric%20date%20encoding). To solve this, our MCP server will provide _targeted tools_ that allow the model to be selective and “clever” in how it accesses data:

- **Selective Data Retrieval:** Instead of returning a whole sheet, tools will return only the needed subset (specific cells, ranges, or summary stats). For instance, a read_range tool can return a small range of cells (e.g. A1:F20) that the LLM explicitly requests, rather than the entire sheet. A filter_data tool can return only rows matching certain criteria, possibly capped at a limit if the result is large. This ensures the model only sees relevant slices of data and stays within context limits.
- **Aggregation and Summary:** The server can perform computations or summaries on behalf of the LLM. For example, an analyze_data tool might compute aggregate statistics (count, sum, average, etc.) for a column or provide a high-level summary of a dataset. By offloading heavy analysis to code, we avoid sending raw data to the LLM. As noted in a case study, larger data chunks “strain constraints such as context window size”[\[2\]](https://blog.langchain.com/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/#:~:text=,like%20Excel%20numeric%20date%20encoding), so pre-computing insights prevents the model from having to ingest massive tables.
- **Iterative/Chunked Access:** If an operation does require scanning a large dataset, the tools can handle it incrementally. For example, a find_data tool could search through a sheet internally and only return the found entries or a count of matches, rather than the model reading everything. In cases where multiple calls are needed (e.g. reading data page by page), the LLM can invoke the tool multiple times, each time advancing an offset or focusing on a different segment. The protocol allows the model to strategize tool use in multiple steps[\[4\]](https://github.com/tuannvm/slack-mcp-client#:~:text=1,conversation%20history%20per%20Slack%20thread)[\[5\]](https://github.com/tuannvm/slack-mcp-client#:~:text=User%3A%20,with%20my%20deployment%20pipeline), maintaining context across calls without ever overloading any single response.
- **Resource References:** MCP supports **resources** (read-only data blobs) that the client can fetch and feed to the model as needed[\[6\]](https://modelcontextprotocol.io/docs/learn/server-concepts#:~:text=)[\[7\]](https://modelcontextprotocol.io/docs/learn/server-concepts#:~:text=%2A%20Direct%20Resources%20,returns%20all%20museums%20in%20Barcelona). In our design, instead of treating the entire spreadsheet as one large resource, we can expose smaller logical resources (for example, a specific sheet or a summary of it). The client (or agent) can then decide to retrieve only those resources that are needed for the prompt. This approach ensures that only relevant information is added to the LLM’s context.

By structuring the API this way, the LLM uses tools to **explore and compute on the data rather than naively reading it all**. This mirrors the strategy used by others when integrating data with LLMs: for instance, CloudQuery’s team built tools for their database that focus on exploration (listing tables, searching schemas) and targeted analysis (executing specific queries) to avoid dumping entire datasets[\[8\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=Data%20Discovery%20Tools%3A)[\[9\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=Data%20Analysis%20Tools%3A). We will apply a similar philosophy for Excel files.

## Tool Design for Spreadsheet Analysis

We envision two classes of tools in our MCP server: **core tools** for low-level file and data operations, and **high-level analytical tools** for common analysis tasks. These tools are defined with JSON schemas (per the MCP spec) so the model knows their usage and parameters[\[10\]](https://modelcontextprotocol.io/docs/learn/server-concepts#:~:text=Tools%20are%20schema,execution%20result%20Example%20tool%20definition)[\[11\]](https://modelcontextprotocol.io/docs/learn/server-concepts#:~:text=name%3A%20,date). The model can discover available tools via the protocol (tools/list) and invoke them as needed (tools/call)[\[12\]](https://modelcontextprotocol.io/docs/learn/server-concepts#:~:text=inputs%20and%20outputs,execution%20result%20Example%20tool%20definition). Below we define the key tools:

### Core Tools (Low-Level Operations)

These are fundamental operations that closely map to spreadsheet actions. They provide the building blocks for more complex tasks:

- **open_workbook** – _Description:_ Open an Excel file and prepare it for operations. _Inputs:_ path (string, path to local Excel file). _Outputs:_ A workbook_id (string/handle) to reference the file in subsequent calls.  
    – This tool will use Excelize to open the file from a local path. The server can maintain a map of workbook_id to loaded excelize.File object. By using a handle, the LLM doesn’t need to pass the file path every time, and the server can keep the file open for efficiency. (In stateless usage, we could omit this and accept a path on each call, but an open/close pattern is more efficient for multi-step workflows.)
- **list_sheets** – _Description:_ List all worksheet names in the open workbook. _Inputs:_ workbook_id. _Output:_ Array of sheet names.  
    – This helps the LLM discover the structure of the file (similar to listing tables in a database). For example, the model might call this first to figure out which sheet contains relevant data.
- **get_sheet_info** – _Description:_ Get basic info about a sheet. _Inputs:_ workbook_id, sheet_name. _Output:_ Metadata like number of rows & columns, maybe header row values (column names).  
    – This gives the LLM a quick overview of a sheet’s content size and schema. If a sheet has 10,000 rows, the model can see that upfront and decide to use filtering or aggregation tools instead of reading all rows.
- **read_range** – _Description:_ Read a specific range of cells from a sheet. _Inputs:_ workbook_id, sheet_name, range (e.g. "A1:C100"). _Output:_ The values in those cells (possibly as an array of rows, or CSV/JSON).  
    – This is a primary way to retrieve actual data. By requiring a range, we force the model to be deliberate about _which portion_ of the spreadsheet it wants. The returned chunk should be reasonably sized (we may enforce a max number of cells). If the model needs more data, it can call read_range in multiple steps or switch to a higher-level query.
- **find_data** – _Description:_ Search for a value or pattern in a sheet. _Inputs:_ workbook_id, sheet_name, query (string or regex), columns (optional specific columns to search). _Output:_ The first N matches or their coordinates (and possibly total count).  
    – This lets the LLM locate relevant rows without scanning the entire sheet in its context. For example, “find ‘ACME Corp’ in Sheet1” might return the cell locations or the rows containing that text. We will limit N (e.g., return first 20 matches) to avoid huge outputs, and include a note like “X more matches found...” if applicable.
- **write_range** – _Description:_ Write data to a specific range in the sheet. _Inputs:_ workbook_id, sheet_name, start_cell, values (2D array of values). _Output:_ Confirmation or number of cells written.  
    – This tool enables updating the spreadsheet or adding content. For instance, the LLM could create a new small table or modify a cell value. We restrict the size of values that can be written in one call (to prevent the model from trying to output an entire large column in one go). For writing large outputs, a better approach would be generating a formula or a rule that the server can apply (which leads to the next tool).
- **apply_formula** – _Description:_ Apply a formula or transformation to a column or range. _Inputs:_ workbook_id, sheet_name, target_range, formula (string or code to apply per row). _Output:_ Success or preview of applied results.  
    – This high-value tool prevents the model from manually enumerating calculated results. For example, instead of the LLM writing out 1000 new values that are a function of other columns, it can instruct apply_formula with something like “=IF(A2>100, \\"High\\", \\"Low\\")” for the target range or provide a lambda-like expression. The server (Excelize) will execute this formula for each relevant row and fill the target cells. Only a summary or small sample of results would be returned to verify correctness, rather than all values.

_(Note:_ The exact input schema for each tool will be defined using JSON Schema via mcp.NewTool or mcp.Tool{ ... } in code[\[13\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=s.AddTool%28mcp.Tool%7B%20Name%3A%20,pattern%20to%20filter%20table%20names). For example, filter_data might have a JSON schema requiring a column name and a condition string.)

### High-Level Analytical Tools

On top of the core tools, we will provide more **semantic, high-level tools** that encapsulate common analysis patterns. These tools can internally use core operations (or direct Excelize calls) to produce useful results:

- **filter_data** – _Description:_ Filter rows in a sheet by conditions. _Inputs:_ workbook_id, sheet_name, conditions (object or string, e.g. column filters like {"Country": "USA", "Sales": "> 1000"}). _Output:_ A subset of data (rows) matching the filter, or possibly a reference to a new temporary resource with those rows.  
    – This tool is essentially a query operation on the spreadsheet’s data. It allows the LLM to narrow down the dataset (e.g., “rows where Country = US and Sales > 1000”). The output will be limited to a manageable size. If the filter yields many rows, we might return the first few and a count or save the results in a temporary sheet that the LLM can then sample from. This controlled filtering ensures the model never blindly sees tens of thousands of rows at once.
- **analyze_data** – _Description:_ Compute summary statistics or basic analysis on a sheet or range. _Inputs:_ workbook_id, sheet_name, analysis_type (e.g. "describe", "distribution", "trend"), plus parameters (like column names or group-by column). _Output:_ Summary report (text or JSON with stats).  
    – This high-level tool performs **descriptive analytics**. For example, if asked to “analyze sales data”, it could return: _count of records, mean/median of the Sales column, top 5 categories by revenue, etc._ Internally, it will use Excelize to iterate through the data or leverage known functions. The result can be a text description or a structured summary that the LLM can then turn into a paragraph. By having this tool, the LLM doesn’t need to retrieve all data to compute these stats itself – the server does it and returns concise facts.
- **generate_insights** – _Description:_ Provide high-level insights or observations from the data, potentially with context. _Inputs:_ workbook_id, sheet_name (or possibly a range or filtered dataset), and maybe questions or objectives (optional prompt about what insights to focus on). _Output:_ A human-readable report of insights.  
    – This tool aims to bridge raw analysis and narrative. It could combine multiple stats or detect patterns (e.g., “Sales have grown 20% year-over-year, with a seasonal peak in Q4. The best-performing product line is X, contributing 30% of total revenue.”). Implementation-wise, this is challenging to do fully in code; initially, we might implement it by computing various metrics and then _using a small LLM call internally to compose a summary_. For example, the server could gather key figures (min, max, trends) and then call an LLM (like GPT-4) with a prompt to summarize those figures into insight text. However, doing so would require care to avoid recursive calls between the main LLM client and the server. As a simpler first step, generate_insights might just collate data (like a mini dashboard: list of key changes or outliers) and leave it to the primary LLM to draw conclusions. In any case, the presence of this tool informs the model that it can request an overarching analysis without spelling out which computations to do – the server will attempt a reasonable analysis.
- **visualize_data** (potential future tool): _Description:_ Create a simple chart or graph for a given range. _Inputs:_ workbook_id, sheet_name, chart_type, data_range, parameters. _Output:_ An image (chart) or a reference to an image file.  
    – While not a priority in the first iteration, this tool would further reduce context load by providing visual summaries. For example, instead of listing numbers, the model could ask for a chart (which the user could see, out of band of the LLM’s text output). We mention this to ensure the architecture can handle binary outputs (images) in the future via MCP resources or base64 encoding.

All these tools are **exposed via the MCP server** to the LLM. They will be documented to the LLM (via the automated tool listing that includes each tool’s name, description, and JSON schema for inputs/outputs[\[14\]](https://modelcontextprotocol.io/docs/learn/server-concepts#:~:text=operations%3A%20Method%20Purpose%20Returns%20,execution%20result%20Example%20tool%20definition)). With this approach, an LLM query like _“What are the key trends in the data for 2021 vs 2022?”_ could lead the LLM to:  
1\. Call list_sheets to identify relevant sheets (e.g., “Sales_2021” and “Sales_2022”).  
2\. Use analyze_data on each sheet to get summary stats per year.  
3\. Possibly use generate_insights by providing those two sheets as context (or simply compile the insight by itself using the stats).  
4\. Finally, formulate an answer to the user, e.g., _“In 2021, sales were \\$X (avg Y per month) and grew 15% to \\$X in 2022, with notably higher Q4 sales in 2022. The trend indicates... etc.”_ – all without ever sending the full raw dataset to the model.

This design ensures **efficiency and context management**: the heavy lifting is done by the server’s tools, and the LLM is free to focus on reasoning and communication. It aligns with best practices in emerging LLM tool use, where models act as orchestrators of functions instead of data processors[\[15\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=infrastructure%20analysis).

## MCP-Go Server Architecture with Excelize

### Server Structure and Core Components

We will build the server using the **MCP-Go** library (by Mark3Labs) which provides a high-level framework to define tools, resources, and run the MCP server[\[16\]](https://mcp-go.dev/getting-started/#:~:text=func%20main%28%29%20,server.WithToolCapabilities%28false%29%2C)[\[17\]](https://mcp-go.dev/getting-started/#:~:text=%2F%2F%20Add%20tool%20handler%20s,helloHandler). The MCP server will consist of:

- **Tool Definitions:** We will define each tool (core and high-level) using mcp.NewTool or by constructing mcp.Tool structs. Each tool is given a name, description, and an input schema (leveraging JSON Schema for validation)[\[10\]](https://modelcontextprotocol.io/docs/learn/server-concepts#:~:text=Tools%20are%20schema,execution%20result%20Example%20tool%20definition). We’ll use Excelize within the tool handlers to implement the logic. For example, the filter_data handler will likely open the workbook (if not already opened), iterate over rows or use Excelize’s row filtering utilities, and build a result dataset to return. The analyze_data handler will compute stats using Go code (summing columns, etc.). Each handler returns either a mcp.ToolResult with data (text, JSON, or binary) or an error.
- **Resource Handling:** If needed for large outputs or file transfer, we can use MCP **resources** to expose spreadsheet content. For instance, if a user explicitly requests to see an entire small sheet, we might register that sheet as a resource (mcp.NewResource("excel://file1/sheetName", ...)) with MIME type text/csv or similar[\[18\]](https://mcp-go.dev/getting-started/#:~:text=resource%20%3A%3D%20mcp.NewResource%28%20,). The client could then fetch it outside of the model’s prompt or feed it in pieces. However, in our initial design, most data exchange will be via tools (which is the active approach). Resources might be used for things like chart images (visualize_data can produce a PNG resource), or if we need to cache a filtered result as a pseudo-file that the model can then ask to read a part of.
- **Server Initialization:** Using server.NewMCPServer(name, version, ...) we create the server instance[\[16\]](https://mcp-go.dev/getting-started/#:~:text=func%20main%28%29%20,server.WithToolCapabilities%28false%29%2C). We will pass options like server.WithToolCapabilities(true) if we want the server to auto-advertise tool schemas, and possibly server.WithLogging() for debugging. We’ll register all the tools via s.AddTool(tool, handlerFunc) as shown in examples[\[19\]](https://mcp-go.dev/getting-started/#:~:text=%2F%2F%20Add%20tool%20tool%20%3A%3D,%29%2C)[\[20\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=func%20main%28%29%20,mcp%22%2C%20%221.0.0%22%2C%20server.WithLogging%28%29%2C). Each tool’s handler will be a function that receives a context and mcp.CallToolRequest (from which it can extract input parameters) and returns a mcp.CallToolResult.
- **Transport:** For development, we can use STDIO transport (server.ServeStdio(s))[\[21\]](https://mcp-go.dev/getting-started/#:~:text=%2F%2F%20Start%20the%20stdio%20server,v%5Cn%22%2C%20err%29%20%7D), which allows the server to communicate via standard I/O (this is useful when an LLM client launches the server as a subprocess). We can also run it as an HTTP server via server.NewStreamableHTTPServer(s).Start(":8080")[\[22\]](https://mcp-go.dev/getting-started/#:~:text=,Process%3A%20%60client.NewInProcessClient%28server), which would allow remote clients to call it. The **Streamable HTTP** or **Server-Sent Events (SSE)** transports can support long-running tool calls and real-time updates if needed[\[22\]](https://mcp-go.dev/getting-started/#:~:text=,Process%3A%20%60client.NewInProcessClient%28server). Since our use case may involve an IDE plugin or an AI agent connecting locally, STDIO or HTTP are both viable. We’ll choose based on integration environment (for example, Anthropics’ Claude in an IDE might prefer STDIO, whereas a custom client might use HTTP).

**Excelize Integration:** The Excelize library will be our workhorse for spreadsheet manipulation. We will create a small wrapper or utility module (e.g., pkg/excelutil) to handle repetitive tasks like opening files, reading cell ranges, etc. Key considerations:

- We will open workbooks in **read-write mode** (Excelize opens files for both reading and potential writing). Each open_workbook call returns a handle – likely we generate a UUID or simple counter-based ID and store the pointer to the excelize.File in a map (e.g., openFiles\[id\] = excelFile). This map will need thread-safety (a mutex or concurrent-safe map) because multiple goroutines (requests) could be accessing it.
- **Concurrency and Thread Safety:** As a general rule, each Excelize File object should not be accessed concurrently by multiple operations without locking. Excelize’s documentation notes certain functions are concurrency-safe (e.g., some read operations)[\[23\]](https://www.reddit.com/r/golang/comments/wtakn5/excelizev2_and_concurrent_writing/#:~:text=In%20short%2C%20it%20appears%20to,File%20objects), but to avoid subtle issues, we will restrict that _each workbook handle is used by one tool call at a time_. Our server, thanks to Go’s goroutines, can easily handle many simultaneous tool calls[\[3\]](https://gyliu513.medium.com/build-and-run-a-go-mcp-server-in-5mins-0ed28e592069#:~:text=,model%20context%20requests%20in%20parallel), but if two calls target the same file, we will implement a simple lock (per file) to serialize access. This ensures thread safety at the cost of some blocking for simultaneous operations on one file (which is acceptable, as an LLM agent usually operates sequentially on one file anyway).
- After operations, we must decide if we keep workbooks open or auto-close them. We will provide a **close_workbook** tool (for completeness) that the client or model can call when done, to release the file from memory and file locks. Additionally, to avoid leaks, the server might close and remove any open file handle that hasn’t been used in a while or after a session finishes. In the first iteration, we can rely on explicit close calls or simply close on program exit; future refinements might use LRU caching or timeouts for open files.
- **Local File Paths Only:** We will restrict file access to local paths for now (as specified). This means the path parameter in open_workbook (or any direct file operation) must point to a file on the server’s host filesystem. Security-wise, we might sandbox to certain directories or enforce that paths are whitelisted, to prevent arbitrary file access. The architecture, however, is designed such that in the future we could introduce remote file access: for example, an open_workbook that accepts a URL (http://... or s3://...) and downloads the file temporarily, or a variant that connects to Google Sheets via API. To prepare for this, our file handling can be abstracted via an interface (e.g., an implementation of a FileProvider that currently only does local disk, but could have other implementations later).

### Parallel Request Handling

Our server will support multiple concurrent requests/sessions. The Go MCP server can handle many connections or tool calls in parallel by virtue of goroutines and the non-blocking design of mcp-go[\[3\]](https://gyliu513.medium.com/build-and-run-a-go-mcp-server-in-5mins-0ed28e592069#:~:text=,model%20context%20requests%20in%20parallel). In practice, if multiple LLM clients connect (or one client makes concurrent calls), the server will spawn goroutines for each tool invocation. We have to ensure our code is safe under these conditions: as discussed, we’ll guard shared resources like the open workbook map. Also, Excelize operations on different files are naturally parallelizable (different File instances can be used concurrently without issue[\[23\]](https://www.reddit.com/r/golang/comments/wtakn5/excelizev2_and_concurrent_writing/#:~:text=In%20short%2C%20it%20appears%20to,File%20objects)), so multiple users working on different files will not block each other. The use of Go makes it “easy to handle many simultaneous connections with low overhead”[\[3\]](https://gyliu513.medium.com/build-and-run-a-go-mcp-server-in-5mins-0ed28e592069#:~:text=,model%20context%20requests%20in%20parallel), aligning with our requirement to support parallel requests.

One area to consider is **LLM client session state**. If an LLM is carrying on a conversation or chain of tool uses, the MCP protocol does not inherently store session-specific context on the server (aside from maybe resource usage). It’s largely stateless per call unless we design otherwise. If needed, we could tag resources or file handles with a session ID so that one user’s temporary data isn’t visible to another. Initially, we assume a single user or a trusted multi-session environment, but the architecture could be extended (for example, naming workbook handles with unique prefixes per session).

### Pluggable LLM Integration (OpenAI, Anthropic, etc.)

We aim to make the system LLM-agnostic, supporting multiple backend models. There are two primary integration scenarios:

- We will **configure providers via LangChain-Go** to allow easy switching. For example, using environment variables or config, one can choose LLM_PROVIDER=openai or LLM_PROVIDER=anthropic. With LangChain-Go, setting up OpenAI might require an API key and model name (e.g., GPT-4), whereas Anthropics via their API (or Bedrock) would require a different client setup (Anthropic API key or AWS credentials). We’ll leverage LangChain-Go’s configuration utilities (for instance, if LangChain-Go supports a unified interface to providers; otherwise, simple if/else in code to pick the right client). The goal is to make the **LLM component pluggable** – so we could even swap in local models or other providers down the line, without changing the tool logic.
- **Agent vs. Direct Mode:** When using OpenAI, we will run in this agent mode (since the model itself isn’t MCP-aware). This is slightly less efficient than Claude’s native tool use because it relies on prompt-based tool descriptions and parsing. However, LangChain’s agent framework is quite capable of multi-step reasoning and tool use[\[27\]](https://github.com/tuannvm/slack-mcp-client#:~:text=Agent%20Mode)[\[4\]](https://github.com/tuannvm/slack-mcp-client#:~:text=1,conversation%20history%20per%20Slack%20thread). We’ll need to craft a good prompt template describing our tools for the OpenAI model (LangChain may do this automatically from tool schemas). We also note from community reports that the “native OpenAI agent in LangChainGo has known issues” and they use a conversational agent workaround[\[32\]](https://github.com/tuannvm/slack-mcp-client#:~:text=); we will test this thoroughly and possibly contribute improvements or use the recommended approach from LangChain-Go docs for reliable tool usage with OpenAI models. The user will primarily interact with the agent (e.g., via a chat interface or an IDE command), and the agent mediates between GPT-4 and our MCP server.

In summary, our architecture separates concerns cleanly: the **MCP server** (Go) handles Excel file operations, and the LLM/agent (which could be Anthropic Claude with built-in support or GPT-4 via LangChain-Go) handles the reasoning and conversation. By using the MCP standard, we ensure any compliant client can interact, and by using LangChain-Go for non-compliant models, we bridge the gap. This flexibility is a high priority so that both OpenAI and Anthropics (Claude through Bedrock or direct) are supported as “plug-and-play” options.

## Detailed Implementation Plan (Pull Request)

Below is a comprehensive plan for implementing the MCP Excel Analysis Server, organized as a series of steps and components. This can serve as a blueprint for the Pull Request, outlining what files/functions to create or modify and the design decisions at each stage:

1. **Project Setup:** Initialize a new Go module (e.g., mcp-excel-server). Add dependencies in go.mod:
2. github.com/mark3labs/mcp-go (for MCP server functionality)[\[33\]](https://mcp-go.dev/getting-started/#:~:text=%22github.com%2Fmark3labs%2Fmcp).
3. github.com/xuri/excelize/v2 (for Excel file handling).
4. github.com/tmc/langchaingo (for optional in-process agent or testing with LLMs).  
    Also, structure the project into packages: perhaps cmd/ for the server main, internal/tools for tool handlers, internal/excel for Excel helper functions, etc.
5. **Define Core Data Structures:** In internal/excel (or similar), define structures and utilities:
6. A map or struct to manage open workbooks, e.g.:

- type WorkbookHandle struct {  
    File \*excelize.File  
    Mutex sync.Mutex // to guard concurrent access if needed  
    Path string  
    }  
    var openWorkbooks map\[string\]\*WorkbookHandle
- Also provide functions like OpenWorkbook(path string) (id string, err error) which opens the file and stores a handle, and CloseWorkbook(id string). This abstraction makes it easier to later change how files are opened (e.g., add caching or remote files). Initially, OpenWorkbook just does excelize.OpenFile(path). Ensure to handle errors (file not found, invalid format) and maybe restrict to allowed directories.

1. If needed, a function to generate unique IDs for workbook handles (could be a simple counter or a UUID).
2. **Implement Core Tool Handlers:** For each core tool identified, write a handler function:
3. open_workbook: Calls OpenWorkbook(path) and returns either an error (if file invalid) or a handle ID as a result (perhaps as plain text or JSON). Mark this tool as possibly **not requiring user pre-approval** since it’s read-only (though if writing later, consider security).
4. list_sheets: Retrieves the handle from openWorkbooks, locks it (if using locking), calls Excelize File.GetSheetList() to get sheet names, returns them (as JSON array in the tool result).
5. get_sheet_info: Uses Excelize to get dimensions. Excelize can get max row/col via SheetDimension or iterate. Also possibly read the first row for headers. Returns a JSON object like {"rows": 10500, "cols": 10, "headers": \["Name","Age",...\]}.
6. read_range: Parse range input (Excelize accepts ranges like "Sheet1!A1:B10" or needs sheet and coordinates). Use GetRows(sheet) or GetCellValue in loops to collect specified cells. Return the data in a structured format. We might return CSV text or JSON array of rows. JSON is more structured (each row as array or as object if headers known). JSON might be better for the LLM to parse logically, but it could also increase token count. We might default to a simple table text unless the schema output allows a structured return. (MCP allows tool results to be text or JSON etc. We could set output as text for simplicity now).
7. find_data: Probably use SearchSheet if Excelize has one (it doesn’t have a direct search, we may need to iterate). For regex or exact match, loop through cells or through rows (which is O(n) but fine for moderate sizes). Stop when we’ve collected, say, 20 matches. Return maybe a JSON with list of {row, col, snippet} for each match, plus a flag if more found.
8. write_range: Use SetCellValue in loops for each provided value. If writing across many cells, consider performance (but since we limit input size, it should be fine). After writing, call File.Save() to persist changes (or we can wait to save until close, but saving on each write ensures data is not lost if session ends unexpectedly). Return a confirmation message or count of cells updated.

Each handler will use the mcp.CallToolRequest API to get parameters safely (e.g., request.RequireString("path") as seen in examples[\[34\]](https://mcp-go.dev/getting-started/#:~:text=func%20helloHandler%28ctx%20context,Error%28%29%29%2C%20nil) and request.GetNumber("row"), etc.), and then create a result via mcp.NewToolResultText(...) or mcp.NewToolResultJSON(...) as appropriate. We should include error handling: if a parameter is missing or of wrong type, return mcp.NewToolResultError("...") with a clear message.

1. **Implement High-Level Tool Handlers:** These will often orchestrate the core operations:
2. filter_data: There are two approaches: (a) perform filtering in Go code, or (b) leverage Excelize by writing an AutoFilter and then reading visible cells. Excelize supports AutoFilters on a range given criteria (though typically for Excel files, not sure if it can apply and extract directly). It might be simpler to manually filter: read rows in streaming fashion (Excelize has Rows, Next() iterator), apply our condition (which could be a simple equality or a comparison parsed from a string). The condition input might be a single expression like "Country = 'US' AND Sales > 1000". We could implement a basic parser or require JSON conditions (which is easier for the model to produce deterministically). For now, implement a simple parser for expressions involving =, >, &lt;, &gt;=, <=, != on numeric or string columns. Using the header row to map column names to indices. Once filtered, we might create a small new sheet with results or just return the first N rows. Considering context, returning too many rows is not good – so plan to cap output. If more rows match, mention it in the output (e.g., “…and 50 more rows not displayed”).
3. analyze_data: Depending on analysis_type, implement a few sub-commands:
    - If "describe": compute count, mean, median, min, max for numeric columns; mode or distinct count for categorical columns; etc. Use Excelize row iteration or the ColumnSorter utility to get values. We have to be careful with data types (Excelize returns strings for cells; we may need to convert to float for numeric columns). We can attempt to infer type by trying to parse numbers. This tool’s output could be a JSON with stats per column, or a nicely formatted text block. JSON structured output might be better for the model to then format into answer (or the model might directly present it). Alternatively, since the model is good at interpreting text, a well-formatted markdown table of summary stats might be useful. We can refine based on early tests.
    - If "distribution": maybe return a histogram or value distribution for a specified column (e.g., frequency of categories, or bin numeric values).
    - If "trend": perhaps detect upward/downward trends in a time-series column or compare two columns. This might require more complex logic or multiple passes (and possibly overlap with generate_insights). To keep it simple, we might implement "trend" for time series as: compute difference between first and last value, maybe month-over-month changes if a date column is present.
    - **Note:** We should document exactly what each analysis_type does, so the LLM knows what to request. E.g., "Use analysis_type=\\"describe\\" to get descriptive statistics." This can go in the tool description.
4. generate_insights: Start with a straightforward approach: combine a few metrics and output text. For example, we could call analyze_data internally for the sheet (or certain columns) and then produce sentences. This might include finding the largest increase or identifying outliers. While a fully AI-generated insight might be ideal, we have to avoid the server calling the LLM recursively in a way that confuses the main client. If we do decide to use an internal LLM call for insight generation, it should be clearly isolated and probably use a smaller context (like prompt = "Given these stats ..., provide a one-paragraph insight"). However, this could increase complexity and cost. As a first iteration, we can implement some simple heuristic insights: e.g., _“Column X has a strong upward trend, increasing by Y% over the period. Column Z has two distinct clusters of values, suggesting ...”_. We will focus on numerical trends and categorical breakdowns. The output will be textual, marked as coming from the tool result (so the model can choose to quote or rephrase it). Over time, this tool can be improved or even use an internal chain with a specialized model prompt.
5. **Register Tools with MCP Server:** In the main.go (or server initialization code):
6. Initialize the server:

- s := server.NewMCPServer("ExcelMCP", "0.1.0",  
    server.WithToolCapabilities(true),  
    server.WithLogging(),  
    server.WithRecovery())
- We enable WithToolCapabilities(true) so that the server will include tool schemas in responses (helpful for certain clients that auto-consume them)[\[35\]](https://mcp-go.dev/getting-started/#:~:text=s%20%3A%3D%20server.NewMCPServer%28%20,server.WithToolCapabilities%28true%29%2C%20server.WithRecovery%28%29%2C%20server.WithHooks%28myHooks%29%2C). WithRecovery() is good to catch panics in handlers. Logging will help debug calls.

1. For each tool, construct the schema. mcp-go offers fluent builders (as in the Hello World example[\[19\]](https://mcp-go.dev/getting-started/#:~:text=%2F%2F%20Add%20tool%20tool%20%3A%3D,%29%2C)) or struct initialization (as CloudQuery did[\[13\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=s.AddTool%28mcp.Tool%7B%20Name%3A%20,pattern%20to%20filter%20table%20names)). We can use whichever is clearer. Example for list_sheets:

- tool := mcp.NewTool("list_sheets",  
    mcp.WithDescription("List all sheets in the Excel workbook"),  
    mcp.WithString("workbook_id", mcp.Required(), mcp.Description("ID of the open workbook"))  
    )  
    s.AddTool(tool, handleListSheets)
- For more complex inputs like filter_data, we might use an object schema:
- filterTool := mcp.NewTool("filter_data",  
    mcp.WithDescription("Filter rows by conditions in a sheet"),  
    mcp.WithString("workbook_id", mcp.Required()),  
    mcp.WithString("sheet", mcp.Required()),  
    mcp.WithObject("conditions", mcp.Required(), mcp.Description("Filter conditions, e.g. {\\"Column\\": \\"Value\\", ...} or expression"))  
    )  
    s.AddTool(filterTool, handleFilterData)
- (Alternatively, define InputSchema as a map manually like CloudQuery did[\[36\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=Name%3A%20,).)

1. Repeat for each tool. Ensure the descriptions are concise and clear for the LLM to understand usage. Use mcp.WithExample() if the library supports providing example inputs.
2. **Resource registration (if any):** If we plan some static resources (like a template or documentation), we could register those via s.AddResource(...). However, in this case our “resources” are dynamic (files the user opens). We might skip explicit NewResource calls in favor of just handling through tools. Perhaps we will add a resource template for file URIs if it made sense (e.g., define a template excel://{file}/{sheet} – but since our files are local and opened via handle, it’s not a fixed URI scheme yet).
3. **Concurrent Processing & Testing:** Write tests or a simple simulation to ensure multiple calls can be handled:
4. For example, use Go routines to simulate two calls: open the same file and read different ranges simultaneously. Verify data integrity and no race conditions (running with -race flag). If issues arise, adjust locking (maybe lock at a finer granularity if needed – e.g., locking per sheet during read might not be necessary if two different sheets in same file are read, but to be safe initial approach locks per file).
5. Also test parallel calls on different files (which should be entirely independent).  
    These tests ensure our **parallel request support** is solid. The Medium article notes Go’s strength here[\[3\]](https://gyliu513.medium.com/build-and-run-a-go-mcp-server-in-5mins-0ed28e592069#:~:text=,model%20context%20requests%20in%20parallel), which we leverage fully.
6. **LLM Integration (LangChain-Go Agent Setup):** Though this might reside in a different component (perhaps in a client or a CLI tool), we want to demonstrate usage with an LLM:
7. Create a small Go program (could be under cmd/agent-test/) that uses langchaingo to instantiate an OpenAI GPT-4 model and wraps it with an agent that knows how to call our MCP server. We can use the **In-Process Transport** for simplicity: client := client.NewInProcessClient(s)[\[37\]](https://mcp-go.dev/getting-started/#:~:text=,Process%3A%20%60client.NewInProcessClient%28server) allows us to have a client handle that calls the server internally (no networking). LangChain-Go might not have built-in MCP agent, so we may have to implement a custom Tool wrapper: essentially, we can implement LangChain’s tools.Tool interface for each MCP tool, where calling it actually calls client.CallTool(...). However, an alternative is to use the useNativeTools setting from the Slack MCP client config[\[38\]](https://github.com/tuannvm/slack-mcp-client#:~:text=,4o), which hints that LangChain-Go can directly use the JSON schemas (the Slack client chooses between “native LangChain tools vs system prompt-based tools”[\[39\]](https://github.com/tuannvm/slack-mcp-client#:~:text=,default%3A%2020)). We will explore LangChain-Go’s documentation: perhaps it can consume an OpenAPI or JSON schema to auto-create tools. If not, we will manually map them.
8. The goal of this step is not necessarily to include in the PR unless this repository will also host a client, but to ensure that our server can interface with an LLM. We might include an example in the README on how to connect OpenAI or Claude to the server.
9. Also ensure environment variable support: e.g., if OPENAI_API_KEY is set, our agent code will pick it up (LangChain-Go’s OpenAI client uses env var for API key by default[\[40\]](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html#:~:text=OpenAI%20%E2%80%94%20LangChain%20documentation%20OpenAI,and%20set%20environment%20variable%20OPENAI_API_KEY)). Similarly for Anthropic (if we test via their API).
10. **Documentation and Examples:** Along with code, write a **README or docs**:
11. Describe how to run the server (e.g., go run cmd/server/main.go).
12. Provide example usage. If using Anthropics: e.g., “Start the server and connect Claude (in platform X) to it. The tools will appear with the given names.” If using OpenAI: “Run the agent test which allows you to ask questions to GPT-4 and it will utilize the server’s tools under the hood.” Possibly provide a sample conversation log demonstrating the LLM calling the tools. This is important for showing that the design indeed prevents context overload. For instance, an example interaction might show GPT-4 asking the server for list_sheets, then read_range of a small range, then analyze_data, etc., before answering the question.
13. Highlight the **security** aspects: The server only accesses files explicitly opened by the user and only at allowed paths. And mention any limitations (e.g., formulas support is basic, insight generation is rudimentary, etc.).
14. Document how to extend to remote files (perhaps as a note: e.g., “To support remote files in the future, one could implement OpenWorkbook to download the file or stream it.”).
15. Document how to add new tools or integrate new LLM providers, to make the project developer-friendly for extension.
16. **Testing & QA:** Before finalizing the PR, perform end-to-end tests:
17. Use a fairly large Excel file (e.g., 5k-10k rows) with known data. Try asking the LLM (via our integration) a question that requires analyzing that data. Verify that the LLM does not time out or get overwhelmed. It should use our tools — we can check server logs for which tools were called. Ensure the answer is correct or at least reasonable given the data.
18. Test edge cases: ask for an extremely large range (the tool should refuse or truncate), ask for a filter that matches nothing (should handle gracefully), open a non-existent file (should return an error message the LLM can relay), use tools out of order (e.g., call list_sheets with an invalid workbook_id).
19. If possible, test with both providers: Claude (through an MCP-compatible client) and GPT-4 (through our agent). This will validate the pluggability. We might find that the prompt needs tuning for GPT-4 to reliably invoke tools – adjust the system prompt or tool descriptions accordingly.
20. **Performance Considerations:** In the PR notes, mention any performance optimizations done or deferred:
    - For example, reading very large sheets row-by-row in Excelize can be slow and memory intensive. If needed, we might use Excelize’s streaming reader for huge files (it provides a way to stream rows to avoid loading whole sheet into memory). Our design can accommodate swapping in a streaming approach inside analyze_data or filter_data if files are massive.
    - We will note that for extremely large data (millions of rows), a different approach (like loading into a database or using Pandas via Python) might be more efficient, but that is beyond scope. Our focus is on moderately large spreadsheets that fit typical business use (thousands to tens of thousands of rows).
    - Also, note that parallel requests on the same file will be bottlenecked by the file lock – but that’s acceptable for now. If we needed to allow truly concurrent reads, we could open a second Excelize File for the same path (since Excelize likely allows two opens on same file in read mode). We choose simplicity (one open copy) unless requirements change.
21. **Future Extensions (noted but not implemented in initial PR):**
    - Remote file support (as discussed, possibly via URLs or cloud storage integration).
    - Authentication/Authorization for file access (if this server was multi-user, we’d need to restrict who can open what – currently assume a single-user or trusted context).
    - More advanced data operations: pivot tables, joining two sheets, etc. (These could be new tools down the line).
    - Enhanced generate_insights using ML/AI on the server side for deeper analysis, or integrating with libraries like Pandas or even AutoML for trend detection.
    - A UI or CLI on top of this – while out of scope for the PR, a simple command-line could let a user type queries and see the model’s answer, to demonstrate functionality.

By following this plan, our PR will deliver a robust MCP server tailored for Excel analysis. The solution will allow LLMs to effectively work with spreadsheet data in a controlled manner – **reducing context overload by focusing on relevant data via specialized tools**[\[2\]](https://blog.langchain.com/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/#:~:text=,like%20Excel%20numeric%20date%20encoding), and leveraging Go’s concurrency to handle multiple requests efficiently[\[3\]](https://gyliu513.medium.com/build-and-run-a-go-mcp-server-in-5mins-0ed28e592069#:~:text=,model%20context%20requests%20in%20parallel). This lays a strong foundation for future improvements and integrations, positioning the project at the forefront of enabling data-aware AI assistants[\[41\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=TL%3BDR%3A%20We%20built%20a%20Go,pattern%20for%20massive%20productivity%20gains)[\[26\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=and%20data%20insights%20directly%20to,our%20actual%20cloud%20infrastructure%20data).

## Conclusion

In this architecture, we have addressed the primary challenges: **context management**, by introducing intelligent tools that summarize or slice data; **tool design**, by defining both granular and high-level operations appropriate for spreadsheet tasks; and **system architecture**, by utilizing Go’s strengths (Excelize for data, MCP-Go for protocol, goroutines for parallelism) and planning for extensibility (pluggable LLMs, future remote files). The result will be an MCP-compliant server that empowers LLMs to interact with Excel files effectively – retrieving insights and answering questions without ever dumping large raw tables into the conversation. This approach not only prevents overload but also aligns with the emerging best practices of making LLMs “tools-aware” for complex data sources[\[8\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=Data%20Discovery%20Tools%3A)[\[9\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=Data%20Analysis%20Tools%3A). The detailed implementation plan ensures a clear path to building and iterating on this capability. With this in place, users (and developers) can harness powerful AI to query and analyze spreadsheets in natural language, backed by the efficiency of code-driven data processing, and do so in a secure, controlled, and scalable manner.

**Sources:** [\[1\]](https://blog.langchain.com/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/#:~:text=When%20I%20first%20tried%20to,we%20get%20a%20decent%20result)[\[2\]](https://blog.langchain.com/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/#:~:text=,like%20Excel%20numeric%20date%20encoding)[\[8\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=Data%20Discovery%20Tools%3A)[\[9\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=Data%20Analysis%20Tools%3A)[\[3\]](https://gyliu513.medium.com/build-and-run-a-go-mcp-server-in-5mins-0ed28e592069#:~:text=,model%20context%20requests%20in%20parallel)

[\[1\]](https://blog.langchain.com/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/#:~:text=When%20I%20first%20tried%20to,we%20get%20a%20decent%20result) [\[2\]](https://blog.langchain.com/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/#:~:text=,like%20Excel%20numeric%20date%20encoding) Summarizing and Querying Data from Excel Spreadsheets Using eparse and a Large Language Model

<https://blog.langchain.com/summarizing-and-querying-data-from-excel-spreadsheets-using-eparse-and-a-large-language-model/>

[\[3\]](https://gyliu513.medium.com/build-and-run-a-go-mcp-server-in-5mins-0ed28e592069#:~:text=,model%20context%20requests%20in%20parallel) Build and Run a Go MCP Server in 5Mins | by Guangya Liu | Medium

<https://gyliu513.medium.com/build-and-run-a-go-mcp-server-in-5mins-0ed28e592069>

[\[4\]](https://github.com/tuannvm/slack-mcp-client#:~:text=1,conversation%20history%20per%20Slack%20thread) [\[5\]](https://github.com/tuannvm/slack-mcp-client#:~:text=User%3A%20,with%20my%20deployment%20pipeline) [\[27\]](https://github.com/tuannvm/slack-mcp-client#:~:text=Agent%20Mode) [\[28\]](https://github.com/tuannvm/slack-mcp-client#:~:text=Standard%20Mode%3A) [\[31\]](https://github.com/tuannvm/slack-mcp-client#:~:text=Agent%20Mode%20enables%20more%20interactive,maintain%20better%20context%20throughout%20conversations) [\[32\]](https://github.com/tuannvm/slack-mcp-client#:~:text=) [\[38\]](https://github.com/tuannvm/slack-mcp-client#:~:text=,4o) [\[39\]](https://github.com/tuannvm/slack-mcp-client#:~:text=,default%3A%2020) GitHub - tuannvm/slack-mcp-client: A Slack bot and MCP client acts as a bridge between Slack and Model Context Protocol (MCP) servers. Using Slack as the interface, it enables large language models (LLMs) to connect and interact with various MCP servers through standardized MCP tools.

<https://github.com/tuannvm/slack-mcp-client>

[\[6\]](https://modelcontextprotocol.io/docs/learn/server-concepts#:~:text=) [\[7\]](https://modelcontextprotocol.io/docs/learn/server-concepts#:~:text=%2A%20Direct%20Resources%20,returns%20all%20museums%20in%20Barcelona) [\[10\]](https://modelcontextprotocol.io/docs/learn/server-concepts#:~:text=Tools%20are%20schema,execution%20result%20Example%20tool%20definition) [\[11\]](https://modelcontextprotocol.io/docs/learn/server-concepts#:~:text=name%3A%20,date) [\[12\]](https://modelcontextprotocol.io/docs/learn/server-concepts#:~:text=inputs%20and%20outputs,execution%20result%20Example%20tool%20definition) [\[14\]](https://modelcontextprotocol.io/docs/learn/server-concepts#:~:text=operations%3A%20Method%20Purpose%20Returns%20,execution%20result%20Example%20tool%20definition) Understanding MCP servers - Model Context Protocol

<https://modelcontextprotocol.io/docs/learn/server-concepts>

[\[8\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=Data%20Discovery%20Tools%3A) [\[9\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=Data%20Analysis%20Tools%3A) [\[13\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=s.AddTool%28mcp.Tool%7B%20Name%3A%20,pattern%20to%20filter%20table%20names) [\[15\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=infrastructure%20analysis) [\[20\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=func%20main%28%29%20,mcp%22%2C%20%221.0.0%22%2C%20server.WithLogging%28%29%2C) [\[24\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=When%20Anthropic%20dropped%20the%20Model,But%20also%20an%20opportunity) [\[25\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=At%20CloudQuery%20%2C%20we%27ve%20been,how%20our%20engineering%20team%20works) [\[26\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=and%20data%20insights%20directly%20to,our%20actual%20cloud%20infrastructure%20data) [\[36\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=Name%3A%20,) [\[41\]](https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server#:~:text=TL%3BDR%3A%20We%20built%20a%20Go,pattern%20for%20massive%20productivity%20gains) How we made our IDEs data-aware with a Go MCP Server | CloudQuery Blog

<https://www.cloudquery.io/blog/how-we-made-our-ides-data-aware-with-a-go-mcp-server>

[\[16\]](https://mcp-go.dev/getting-started/#:~:text=func%20main%28%29%20,server.WithToolCapabilities%28false%29%2C) [\[17\]](https://mcp-go.dev/getting-started/#:~:text=%2F%2F%20Add%20tool%20handler%20s,helloHandler) [\[18\]](https://mcp-go.dev/getting-started/#:~:text=resource%20%3A%3D%20mcp.NewResource%28%20,) [\[19\]](https://mcp-go.dev/getting-started/#:~:text=%2F%2F%20Add%20tool%20tool%20%3A%3D,%29%2C) [\[21\]](https://mcp-go.dev/getting-started/#:~:text=%2F%2F%20Start%20the%20stdio%20server,v%5Cn%22%2C%20err%29%20%7D) [\[22\]](https://mcp-go.dev/getting-started/#:~:text=,Process%3A%20%60client.NewInProcessClient%28server) [\[33\]](https://mcp-go.dev/getting-started/#:~:text=%22github.com%2Fmark3labs%2Fmcp) [\[34\]](https://mcp-go.dev/getting-started/#:~:text=func%20helloHandler%28ctx%20context,Error%28%29%29%2C%20nil) [\[35\]](https://mcp-go.dev/getting-started/#:~:text=s%20%3A%3D%20server.NewMCPServer%28%20,server.WithToolCapabilities%28true%29%2C%20server.WithRecovery%28%29%2C%20server.WithHooks%28myHooks%29%2C) [\[37\]](https://mcp-go.dev/getting-started/#:~:text=,Process%3A%20%60client.NewInProcessClient%28server) Getting Started – MCP-Go

<https://mcp-go.dev/getting-started/>

[\[23\]](https://www.reddit.com/r/golang/comments/wtakn5/excelizev2_and_concurrent_writing/#:~:text=In%20short%2C%20it%20appears%20to,File%20objects) Excelize/v2 and Concurrent Writing? : r/golang - Reddit

<https://www.reddit.com/r/golang/comments/wtakn5/excelizev2_and_concurrent_writing/>

[\[29\]](https://github.com/tmc/langchaingo#:~:text=import%20%28%20) [\[30\]](https://github.com/tmc/langchaingo#:~:text=ctx%20%3A%3D%20context,if%20err%20%21%3D%20nil) GitHub - tmc/langchaingo: LangChain for Go, the easiest way to write LLM-based programs in Go

<https://github.com/tmc/langchaingo>

[\[40\]](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html#:~:text=OpenAI%20%E2%80%94%20LangChain%20documentation%20OpenAI,and%20set%20environment%20variable%20OPENAI_API_KEY) OpenAI — LangChain documentation

<https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html>